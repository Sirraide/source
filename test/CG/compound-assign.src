// R       %srcc --ir --short-filenames --target x86_64-unknown-linux %s
// R[//U]  %srcc --ir -fno-overflow-checks --target x86_64-unknown-linux %s
// R[//L]  %srcc --llvm --short-filenames --target x86_64-unknown-linux %s
// R[//UL] %srcc --llvm -fno-overflow-checks --target x86_64-unknown-linux %s
program test;

proc x (int a, in int b) {
    a += b;
    a +~= b;
    a -= b;
    a -~= b;
    a *= b;
    a *~= b;
    a /= b;
    a %= b;
    a <<= b;
    a >>= b;
    a <<<= b;
    a >>>= b;
}

// * @0 = "<<<\00"
// + @1 = "shift amount exceeds bit width\00"
// + @2 = "<<\00"
// + @3 = "%\00"
// + @4 = "division by zero\00"
// + @5 = "/\00"
// + @6 = "*\00"
// + @7 = "-\00"
// + @8 = "compound-assign.src\00"
// + @9 = "integer overflow\00"
// + @10 = "+\00"
// +
// + proc __src_assert_fail (ptr) external fastcc;
// +
// + proc __src_int_arith_error (ptr) external fastcc;
// +
// + proc __src_main external fastcc {
// + entry:
// +     ret
// + }
// +
// + proc _S1xFvixiE (i64 %0, i64 %1) private fastcc {
// +     #0 = 8, align 8
// +     #1 = 8, align 8
// +     #2 = 64, align 8
// +
// + entry:
// +     store #0, i64 %0, align 8
// +     store #1, i64 %1, align 8
// +     %2 = load i64, #0, align 8
// +     %3 = load i64, #1, align 8
// +     %4 = sadd ov i64 %2, %3
// +     br %4:1 to bb1 else bb2
// +
// + bb1:
// +     store #2, ptr @8, align 8
// +     %5 = ptradd #2, 8
// +     store %5, i64 19, align 8
// +     %6 = ptradd #2, 16
// +     store %6, i64 8, align 8
// +     %7 = ptradd #2, 24
// +     store %7, i64 5, align 8
// +     %8 = ptradd #2, 32
// +     store %8, ptr @10, align 8
// +     %9 = ptradd %8, 8
// +     store %9, i64 1, align 8
// +     %10 = ptradd #2, 48
// +     store %10, ptr @9, align 8
// +     %11 = ptradd %10, 8
// +     store %11, i64 16, align 8
// +     abort at loc("compound-assign.src":8:5) arithmetic(ptr #2)
// +
// + bb2:
// +     store #0, i64 %4:0, align 8
// +     %12 = load i64, #0, align 8
// +     %13 = load i64, #1, align 8
// +     %14 = add i64 %12, %13
// +     store #0, i64 %14, align 8
// +     %15 = load i64, #0, align 8
// +     %16 = load i64, #1, align 8
// +     %17 = ssub ov i64 %15, %16
// +     br %17:1 to bb3 else bb4
// +
// + bb3:
// +     store #2, ptr @8, align 8
// +     %18 = ptradd #2, 8
// +     store %18, i64 19, align 8
// +     %19 = ptradd #2, 16
// +     store %19, i64 10, align 8
// +     %20 = ptradd #2, 24
// +     store %20, i64 5, align 8
// +     %21 = ptradd #2, 32
// +     store %21, ptr @7, align 8
// +     %22 = ptradd %21, 8
// +     store %22, i64 1, align 8
// +     %23 = ptradd #2, 48
// +     store %23, ptr @9, align 8
// +     %24 = ptradd %23, 8
// +     store %24, i64 16, align 8
// +     abort at loc("compound-assign.src":10:5) arithmetic(ptr #2)
// +
// + bb4:
// +     store #0, i64 %17:0, align 8
// +     %25 = load i64, #0, align 8
// +     %26 = load i64, #1, align 8
// +     %27 = sub i64 %25, %26
// +     store #0, i64 %27, align 8
// +     %28 = load i64, #0, align 8
// +     %29 = load i64, #1, align 8
// +     %30 = smul ov i64 %28, %29
// +     br %30:1 to bb5 else bb6
// +
// + bb5:
// +     store #2, ptr @8, align 8
// +     %31 = ptradd #2, 8
// +     store %31, i64 19, align 8
// +     %32 = ptradd #2, 16
// +     store %32, i64 12, align 8
// +     %33 = ptradd #2, 24
// +     store %33, i64 5, align 8
// +     %34 = ptradd #2, 32
// +     store %34, ptr @6, align 8
// +     %35 = ptradd %34, 8
// +     store %35, i64 1, align 8
// +     %36 = ptradd #2, 48
// +     store %36, ptr @9, align 8
// +     %37 = ptradd %36, 8
// +     store %37, i64 16, align 8
// +     abort at loc("compound-assign.src":12:5) arithmetic(ptr #2)
// +
// + bb6:
// +     store #0, i64 %30:0, align 8
// +     %38 = load i64, #0, align 8
// +     %39 = load i64, #1, align 8
// +     %40 = mul i64 %38, %39
// +     store #0, i64 %40, align 8
// +     %41 = load i64, #0, align 8
// +     %42 = load i64, #1, align 8
// +     %43 = icmp eq i64 %42, 0
// +     br %43 to bb7 else bb8
// +
// + bb7:
// +     store #2, ptr @8, align 8
// +     %44 = ptradd #2, 8
// +     store %44, i64 19, align 8
// +     %45 = ptradd #2, 16
// +     store %45, i64 14, align 8
// +     %46 = ptradd #2, 24
// +     store %46, i64 5, align 8
// +     %47 = ptradd #2, 32
// +     store %47, ptr @5, align 8
// +     %48 = ptradd %47, 8
// +     store %48, i64 1, align 8
// +     %49 = ptradd #2, 48
// +     store %49, ptr @4, align 8
// +     %50 = ptradd %49, 8
// +     store %50, i64 16, align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #2)
// +
// + bb8:
// +     %51 = icmp eq i64 %41, -9223372036854775808
// +     %52 = icmp eq i64 %42, -1
// +     %53 = and i1 %51, %52
// +     br %53 to bb9 else bb10
// +
// + bb9:
// +     store #2, ptr @8, align 8
// +     %54 = ptradd #2, 8
// +     store %54, i64 19, align 8
// +     %55 = ptradd #2, 16
// +     store %55, i64 14, align 8
// +     %56 = ptradd #2, 24
// +     store %56, i64 5, align 8
// +     %57 = ptradd #2, 32
// +     store %57, ptr @5, align 8
// +     %58 = ptradd %57, 8
// +     store %58, i64 1, align 8
// +     %59 = ptradd #2, 48
// +     store %59, ptr @9, align 8
// +     %60 = ptradd %59, 8
// +     store %60, i64 16, align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #2)
// +
// + bb10:
// +     %61 = sdiv i64 %41, %42
// +     store #0, i64 %61, align 8
// +     %62 = load i64, #0, align 8
// +     %63 = load i64, #1, align 8
// +     %64 = icmp eq i64 %63, 0
// +     br %64 to bb11 else bb12
// +
// + bb11:
// +     store #2, ptr @8, align 8
// +     %65 = ptradd #2, 8
// +     store %65, i64 19, align 8
// +     %66 = ptradd #2, 16
// +     store %66, i64 15, align 8
// +     %67 = ptradd #2, 24
// +     store %67, i64 5, align 8
// +     %68 = ptradd #2, 32
// +     store %68, ptr @3, align 8
// +     %69 = ptradd %68, 8
// +     store %69, i64 1, align 8
// +     %70 = ptradd #2, 48
// +     store %70, ptr @4, align 8
// +     %71 = ptradd %70, 8
// +     store %71, i64 16, align 8
// +     abort at loc("compound-assign.src":15:5) arithmetic(ptr #2)
// +
// + bb12:
// +     %72 = icmp eq i64 %62, -9223372036854775808
// +     %73 = icmp eq i64 %63, -1
// +     %74 = and i1 %72, %73
// +     br %74 to bb13 else bb14
// +
// + bb13:
// +     store #2, ptr @8, align 8
// +     %75 = ptradd #2, 8
// +     store %75, i64 19, align 8
// +     %76 = ptradd #2, 16
// +     store %76, i64 15, align 8
// +     %77 = ptradd #2, 24
// +     store %77, i64 5, align 8
// +     %78 = ptradd #2, 32
// +     store %78, ptr @3, align 8
// +     %79 = ptradd %78, 8
// +     store %79, i64 1, align 8
// +     %80 = ptradd #2, 48
// +     store %80, ptr @9, align 8
// +     %81 = ptradd %80, 8
// +     store %81, i64 16, align 8
// +     abort at loc("compound-assign.src":15:5) arithmetic(ptr #2)
// +
// + bb14:
// +     %82 = srem i64 %62, %63
// +     store #0, i64 %82, align 8
// +     %83 = load i64, #0, align 8
// +     %84 = load i64, #1, align 8
// +     %85 = icmp uge i64 %84, 64
// +     br %85 to bb15 else bb16
// +
// + bb15:
// +     store #2, ptr @8, align 8
// +     %86 = ptradd #2, 8
// +     store %86, i64 19, align 8
// +     %87 = ptradd #2, 16
// +     store %87, i64 16, align 8
// +     %88 = ptradd #2, 24
// +     store %88, i64 5, align 8
// +     %89 = ptradd #2, 32
// +     store %89, ptr @2, align 8
// +     %90 = ptradd %89, 8
// +     store %90, i64 2, align 8
// +     %91 = ptradd #2, 48
// +     store %91, ptr @1, align 8
// +     %92 = ptradd %91, 8
// +     store %92, i64 30, align 8
// +     abort at loc("compound-assign.src":16:5) arithmetic(ptr #2)
// +
// + bb16:
// +     %93 = shl i64 %83, %84
// +     %94 = ashr i64 %83, 63
// +     %95 = ashr i64 %93, 63
// +     %96 = icmp ne i64 %94, %95
// +     br %96 to bb17 else bb18
// +
// + bb17:
// +     store #2, ptr @8, align 8
// +     %97 = ptradd #2, 8
// +     store %97, i64 19, align 8
// +     %98 = ptradd #2, 16
// +     store %98, i64 16, align 8
// +     %99 = ptradd #2, 24
// +     store %99, i64 5, align 8
// +     %100 = ptradd #2, 32
// +     store %100, ptr @2, align 8
// +     %101 = ptradd %100, 8
// +     store %101, i64 2, align 8
// +     %102 = ptradd #2, 48
// +     store %102, ptr @9, align 8
// +     %103 = ptradd %102, 8
// +     store %103, i64 16, align 8
// +     abort at loc("compound-assign.src":16:5) arithmetic(ptr #2)
// +
// + bb18:
// +     store #0, i64 %93, align 8
// +     %104 = load i64, #0, align 8
// +     %105 = load i64, #1, align 8
// +     %106 = ashr i64 %104, %105
// +     store #0, i64 %106, align 8
// +     %107 = load i64, #0, align 8
// +     %108 = load i64, #1, align 8
// +     %109 = icmp uge i64 %108, 64
// +     br %109 to bb19 else bb20
// +
// + bb19:
// +     store #2, ptr @8, align 8
// +     %110 = ptradd #2, 8
// +     store %110, i64 19, align 8
// +     %111 = ptradd #2, 16
// +     store %111, i64 18, align 8
// +     %112 = ptradd #2, 24
// +     store %112, i64 5, align 8
// +     %113 = ptradd #2, 32
// +     store %113, ptr @0, align 8
// +     %114 = ptradd %113, 8
// +     store %114, i64 3, align 8
// +     %115 = ptradd #2, 48
// +     store %115, ptr @1, align 8
// +     %116 = ptradd %115, 8
// +     store %116, i64 30, align 8
// +     abort at loc("compound-assign.src":18:5) arithmetic(ptr #2)
// +
// + bb20:
// +     %117 = shl i64 %107, %108
// +     store #0, i64 %117, align 8
// +     %118 = load i64, #0, align 8
// +     %119 = load i64, #1, align 8
// +     %120 = lshr i64 %118, %119
// +     store #0, i64 %120, align 8
// +     ret
// + }

//U * proc __src_assert_fail (ptr) external fastcc;
//U +
//U + proc __src_int_arith_error (ptr) external fastcc;
//U +
//U + proc __src_main external fastcc {
//U + entry:
//U +     ret
//U + }
//U +
//U + proc _S1xFvixiE (i64 %0, i64 %1) private fastcc {
//U +     #0 = 8, align 8
//U +     #1 = 8, align 8
//U +
//U + entry:
//U +     store #0, i64 %0, align 8
//U +     store #1, i64 %1, align 8
//U +     %2 = load i64, #0, align 8
//U +     %3 = load i64, #1, align 8
//U +     %4 = add i64 %2, %3
//U +     store #0, i64 %4, align 8
//U +     %5 = load i64, #0, align 8
//U +     %6 = load i64, #1, align 8
//U +     %7 = add i64 %5, %6
//U +     store #0, i64 %7, align 8
//U +     %8 = load i64, #0, align 8
//U +     %9 = load i64, #1, align 8
//U +     %10 = sub i64 %8, %9
//U +     store #0, i64 %10, align 8
//U +     %11 = load i64, #0, align 8
//U +     %12 = load i64, #1, align 8
//U +     %13 = sub i64 %11, %12
//U +     store #0, i64 %13, align 8
//U +     %14 = load i64, #0, align 8
//U +     %15 = load i64, #1, align 8
//U +     %16 = mul i64 %14, %15
//U +     store #0, i64 %16, align 8
//U +     %17 = load i64, #0, align 8
//U +     %18 = load i64, #1, align 8
//U +     %19 = mul i64 %17, %18
//U +     store #0, i64 %19, align 8
//U +     %20 = load i64, #0, align 8
//U +     %21 = load i64, #1, align 8
//U +     %22 = sdiv i64 %20, %21
//U +     store #0, i64 %22, align 8
//U +     %23 = load i64, #0, align 8
//U +     %24 = load i64, #1, align 8
//U +     %25 = srem i64 %23, %24
//U +     store #0, i64 %25, align 8
//U +     %26 = load i64, #0, align 8
//U +     %27 = load i64, #1, align 8
//U +     %28 = shl i64 %26, %27
//U +     store #0, i64 %28, align 8
//U +     %29 = load i64, #0, align 8
//U +     %30 = load i64, #1, align 8
//U +     %31 = ashr i64 %29, %30
//U +     store #0, i64 %31, align 8
//U +     %32 = load i64, #0, align 8
//U +     %33 = load i64, #1, align 8
//U +     %34 = shl i64 %32, %33
//U +     store #0, i64 %34, align 8
//U +     %35 = load i64, #0, align 8
//U +     %36 = load i64, #1, align 8
//U +     %37 = lshr i64 %35, %36
//U +     store #0, i64 %37, align 8
//U +     ret
//U + }

//L * ; ModuleID = 'test'
//L + source_filename = "test"
//L + target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
//L + target triple = "x86_64-unknown-linux-gnu"
//L +
//L + @__srcc_str.10 = private constant [4 x i8] c"<<<\00", align 1
//L + @__srcc_str.9 = private constant [31 x i8] c"shift amount exceeds bit width\00", align 1
//L + @__srcc_str.8 = private constant [3 x i8] c"<<\00", align 1
//L + @__srcc_str.7 = private constant [2 x i8] c"%\00", align 1
//L + @__srcc_str.6 = private constant [17 x i8] c"division by zero\00", align 1
//L + @__srcc_str.5 = private constant [2 x i8] c"/\00", align 1
//L + @__srcc_str.4 = private constant [2 x i8] c"*\00", align 1
//L + @__srcc_str.3 = private constant [2 x i8] c"-\00", align 1
//L + @__srcc_str.2 = private constant [20 x i8] c"compound-assign.src\00", align 1
//L + @__srcc_str.1 = private constant [17 x i8] c"integer overflow\00", align 1
//L + @__srcc_str.0 = private constant [2 x i8] c"+\00", align 1
//L +
//L + ; Function Attrs: nounwind
//L + declare fastcc void @__src_assert_fail(ptr) #0
//L +
//L + ; Function Attrs: nounwind
//L + declare fastcc void @__src_int_arith_error(ptr) #0
//L +
//L + ; Function Attrs: nounwind
//L + define fastcc void @__src_main() #0 {
//L +   ret void
//L + }
//L +
//L + ; Function Attrs: nounwind
//L + define private fastcc void @_S1xFvixiE(i64 %0, i64 %1) #0 {
//L +   %3 = alloca i8, i64 8, align 8
//L +   %4 = alloca i8, i64 8, align 8
//L +   %5 = alloca i8, i64 64, align 8
//L +   store i64 %0, ptr %3, align 8
//L +   store i64 %1, ptr %4, align 8
//L +   %6 = load i64, ptr %3, align 8
//L +   %7 = load i64, ptr %4, align 8
//L +   %8 = call { i64, i1 } @llvm.sadd.with.overflow.i64(i64 %6, i64 %7)
//L +   %9 = extractvalue { i64, i1 } %8, 0
//L +   %10 = extractvalue { i64, i1 } %8, 1
//L +   br i1 %10, label %11, label %19
//L +
//L + 11:                                               ; preds = %2
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %12 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %12, align 8
//L +   %13 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 8, ptr %13, align 8
//L +   %14 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %14, align 8
//L +   %15 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.0, ptr %15, align 8
//L +   %16 = getelementptr inbounds nuw i8, ptr %15, i32 8
//L +   store i64 1, ptr %16, align 8
//L +   %17 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %17, align 8
//L +   %18 = getelementptr inbounds nuw i8, ptr %17, i32 8
//L +   store i64 16, ptr %18, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 19:                                               ; preds = %2
//L +   store i64 %9, ptr %3, align 8
//L +   %20 = load i64, ptr %3, align 8
//L +   %21 = load i64, ptr %4, align 8
//L +   %22 = add i64 %20, %21
//L +   store i64 %22, ptr %3, align 8
//L +   %23 = load i64, ptr %3, align 8
//L +   %24 = load i64, ptr %4, align 8
//L +   %25 = call { i64, i1 } @llvm.ssub.with.overflow.i64(i64 %23, i64 %24)
//L +   %26 = extractvalue { i64, i1 } %25, 0
//L +   %27 = extractvalue { i64, i1 } %25, 1
//L +   br i1 %27, label %28, label %36
//L +
//L + 28:                                               ; preds = %19
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %29 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %29, align 8
//L +   %30 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 10, ptr %30, align 8
//L +   %31 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %31, align 8
//L +   %32 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.3, ptr %32, align 8
//L +   %33 = getelementptr inbounds nuw i8, ptr %32, i32 8
//L +   store i64 1, ptr %33, align 8
//L +   %34 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %34, align 8
//L +   %35 = getelementptr inbounds nuw i8, ptr %34, i32 8
//L +   store i64 16, ptr %35, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 36:                                               ; preds = %19
//L +   store i64 %26, ptr %3, align 8
//L +   %37 = load i64, ptr %3, align 8
//L +   %38 = load i64, ptr %4, align 8
//L +   %39 = sub i64 %37, %38
//L +   store i64 %39, ptr %3, align 8
//L +   %40 = load i64, ptr %3, align 8
//L +   %41 = load i64, ptr %4, align 8
//L +   %42 = call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %40, i64 %41)
//L +   %43 = extractvalue { i64, i1 } %42, 0
//L +   %44 = extractvalue { i64, i1 } %42, 1
//L +   br i1 %44, label %45, label %53
//L +
//L + 45:                                               ; preds = %36
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %46 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %46, align 8
//L +   %47 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 12, ptr %47, align 8
//L +   %48 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %48, align 8
//L +   %49 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.4, ptr %49, align 8
//L +   %50 = getelementptr inbounds nuw i8, ptr %49, i32 8
//L +   store i64 1, ptr %50, align 8
//L +   %51 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %51, align 8
//L +   %52 = getelementptr inbounds nuw i8, ptr %51, i32 8
//L +   store i64 16, ptr %52, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 53:                                               ; preds = %36
//L +   store i64 %43, ptr %3, align 8
//L +   %54 = load i64, ptr %3, align 8
//L +   %55 = load i64, ptr %4, align 8
//L +   %56 = mul i64 %54, %55
//L +   store i64 %56, ptr %3, align 8
//L +   %57 = load i64, ptr %3, align 8
//L +   %58 = load i64, ptr %4, align 8
//L +   %59 = icmp eq i64 %58, 0
//L +   br i1 %59, label %60, label %68
//L +
//L + 60:                                               ; preds = %53
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %61 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %61, align 8
//L +   %62 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 14, ptr %62, align 8
//L +   %63 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %63, align 8
//L +   %64 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.5, ptr %64, align 8
//L +   %65 = getelementptr inbounds nuw i8, ptr %64, i32 8
//L +   store i64 1, ptr %65, align 8
//L +   %66 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.6, ptr %66, align 8
//L +   %67 = getelementptr inbounds nuw i8, ptr %66, i32 8
//L +   store i64 16, ptr %67, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 68:                                               ; preds = %53
//L +   %69 = icmp eq i64 %57, -9223372036854775808
//L +   %70 = icmp eq i64 %58, -1
//L +   %71 = and i1 %69, %70
//L +   br i1 %71, label %72, label %80
//L +
//L + 72:                                               ; preds = %68
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %73 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %73, align 8
//L +   %74 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 14, ptr %74, align 8
//L +   %75 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %75, align 8
//L +   %76 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.5, ptr %76, align 8
//L +   %77 = getelementptr inbounds nuw i8, ptr %76, i32 8
//L +   store i64 1, ptr %77, align 8
//L +   %78 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %78, align 8
//L +   %79 = getelementptr inbounds nuw i8, ptr %78, i32 8
//L +   store i64 16, ptr %79, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 80:                                               ; preds = %68
//L +   %81 = sdiv i64 %57, %58
//L +   store i64 %81, ptr %3, align 8
//L +   %82 = load i64, ptr %3, align 8
//L +   %83 = load i64, ptr %4, align 8
//L +   %84 = icmp eq i64 %83, 0
//L +   br i1 %84, label %85, label %93
//L +
//L + 85:                                               ; preds = %80
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %86 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %86, align 8
//L +   %87 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 15, ptr %87, align 8
//L +   %88 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %88, align 8
//L +   %89 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.7, ptr %89, align 8
//L +   %90 = getelementptr inbounds nuw i8, ptr %89, i32 8
//L +   store i64 1, ptr %90, align 8
//L +   %91 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.6, ptr %91, align 8
//L +   %92 = getelementptr inbounds nuw i8, ptr %91, i32 8
//L +   store i64 16, ptr %92, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 93:                                               ; preds = %80
//L +   %94 = icmp eq i64 %82, -9223372036854775808
//L +   %95 = icmp eq i64 %83, -1
//L +   %96 = and i1 %94, %95
//L +   br i1 %96, label %97, label %105
//L +
//L + 97:                                               ; preds = %93
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %98 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %98, align 8
//L +   %99 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 15, ptr %99, align 8
//L +   %100 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %100, align 8
//L +   %101 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.7, ptr %101, align 8
//L +   %102 = getelementptr inbounds nuw i8, ptr %101, i32 8
//L +   store i64 1, ptr %102, align 8
//L +   %103 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %103, align 8
//L +   %104 = getelementptr inbounds nuw i8, ptr %103, i32 8
//L +   store i64 16, ptr %104, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 105:                                              ; preds = %93
//L +   %106 = srem i64 %82, %83
//L +   store i64 %106, ptr %3, align 8
//L +   %107 = load i64, ptr %3, align 8
//L +   %108 = load i64, ptr %4, align 8
//L +   %109 = icmp uge i64 %108, 64
//L +   br i1 %109, label %110, label %118
//L +
//L + 110:                                              ; preds = %105
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %111 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %111, align 8
//L +   %112 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 16, ptr %112, align 8
//L +   %113 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %113, align 8
//L +   %114 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.8, ptr %114, align 8
//L +   %115 = getelementptr inbounds nuw i8, ptr %114, i32 8
//L +   store i64 2, ptr %115, align 8
//L +   %116 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.9, ptr %116, align 8
//L +   %117 = getelementptr inbounds nuw i8, ptr %116, i32 8
//L +   store i64 30, ptr %117, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 118:                                              ; preds = %105
//L +   %119 = shl i64 %107, %108
//L +   %120 = ashr i64 %107, 63
//L +   %121 = ashr i64 %119, 63
//L +   %122 = icmp ne i64 %120, %121
//L +   br i1 %122, label %123, label %131
//L +
//L + 123:                                              ; preds = %118
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %124 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %124, align 8
//L +   %125 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 16, ptr %125, align 8
//L +   %126 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %126, align 8
//L +   %127 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.8, ptr %127, align 8
//L +   %128 = getelementptr inbounds nuw i8, ptr %127, i32 8
//L +   store i64 2, ptr %128, align 8
//L +   %129 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %129, align 8
//L +   %130 = getelementptr inbounds nuw i8, ptr %129, i32 8
//L +   store i64 16, ptr %130, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 131:                                              ; preds = %118
//L +   store i64 %119, ptr %3, align 8
//L +   %132 = load i64, ptr %3, align 8
//L +   %133 = load i64, ptr %4, align 8
//L +   %134 = ashr i64 %132, %133
//L +   store i64 %134, ptr %3, align 8
//L +   %135 = load i64, ptr %3, align 8
//L +   %136 = load i64, ptr %4, align 8
//L +   %137 = icmp uge i64 %136, 64
//L +   br i1 %137, label %138, label %146
//L +
//L + 138:                                              ; preds = %131
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %139 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %139, align 8
//L +   %140 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 18, ptr %140, align 8
//L +   %141 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %141, align 8
//L +   %142 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.10, ptr %142, align 8
//L +   %143 = getelementptr inbounds nuw i8, ptr %142, i32 8
//L +   store i64 3, ptr %143, align 8
//L +   %144 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.9, ptr %144, align 8
//L +   %145 = getelementptr inbounds nuw i8, ptr %144, i32 8
//L +   store i64 30, ptr %145, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 146:                                              ; preds = %131
//L +   %147 = shl i64 %135, %136
//L +   store i64 %147, ptr %3, align 8
//L +   %148 = load i64, ptr %3, align 8
//L +   %149 = load i64, ptr %4, align 8
//L +   %150 = lshr i64 %148, %149
//L +   store i64 %150, ptr %3, align 8
//L +   ret void
//L + }
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.sadd.with.overflow.i64(i64, i64) #1
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.ssub.with.overflow.i64(i64, i64) #1
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.smul.with.overflow.i64(i64, i64) #1
//L +
//L + attributes #0 = { nounwind }
//L + attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
//L +
//L + !llvm.module.flags = !{!0}
//L +
//L + !0 = !{i32 2, !"Debug Info Version", i32 3}

//UL * ; ModuleID = 'test'
//UL + source_filename = "test"
//UL + target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
//UL + target triple = "x86_64-unknown-linux-gnu"
//UL +
//UL + ; Function Attrs: nounwind
//UL + declare fastcc void @__src_assert_fail(ptr) #0
//UL +
//UL + ; Function Attrs: nounwind
//UL + declare fastcc void @__src_int_arith_error(ptr) #0
//UL +
//UL + ; Function Attrs: nounwind
//UL + define fastcc void @__src_main() #0 {
//UL +   ret void
//UL + }
//UL +
//UL + ; Function Attrs: nounwind
//UL + define private fastcc void @_S1xFvixiE(i64 %0, i64 %1) #0 {
//UL +   %3 = alloca i8, i64 8, align 8
//UL +   %4 = alloca i8, i64 8, align 8
//UL +   store i64 %0, ptr %3, align 8
//UL +   store i64 %1, ptr %4, align 8
//UL +   %5 = load i64, ptr %3, align 8
//UL +   %6 = load i64, ptr %4, align 8
//UL +   %7 = add i64 %5, %6
//UL +   store i64 %7, ptr %3, align 8
//UL +   %8 = load i64, ptr %3, align 8
//UL +   %9 = load i64, ptr %4, align 8
//UL +   %10 = add i64 %8, %9
//UL +   store i64 %10, ptr %3, align 8
//UL +   %11 = load i64, ptr %3, align 8
//UL +   %12 = load i64, ptr %4, align 8
//UL +   %13 = sub i64 %11, %12
//UL +   store i64 %13, ptr %3, align 8
//UL +   %14 = load i64, ptr %3, align 8
//UL +   %15 = load i64, ptr %4, align 8
//UL +   %16 = sub i64 %14, %15
//UL +   store i64 %16, ptr %3, align 8
//UL +   %17 = load i64, ptr %3, align 8
//UL +   %18 = load i64, ptr %4, align 8
//UL +   %19 = mul i64 %17, %18
//UL +   store i64 %19, ptr %3, align 8
//UL +   %20 = load i64, ptr %3, align 8
//UL +   %21 = load i64, ptr %4, align 8
//UL +   %22 = mul i64 %20, %21
//UL +   store i64 %22, ptr %3, align 8
//UL +   %23 = load i64, ptr %3, align 8
//UL +   %24 = load i64, ptr %4, align 8
//UL +   %25 = sdiv i64 %23, %24
//UL +   store i64 %25, ptr %3, align 8
//UL +   %26 = load i64, ptr %3, align 8
//UL +   %27 = load i64, ptr %4, align 8
//UL +   %28 = srem i64 %26, %27
//UL +   store i64 %28, ptr %3, align 8
//UL +   %29 = load i64, ptr %3, align 8
//UL +   %30 = load i64, ptr %4, align 8
//UL +   %31 = shl i64 %29, %30
//UL +   store i64 %31, ptr %3, align 8
//UL +   %32 = load i64, ptr %3, align 8
//UL +   %33 = load i64, ptr %4, align 8
//UL +   %34 = ashr i64 %32, %33
//UL +   store i64 %34, ptr %3, align 8
//UL +   %35 = load i64, ptr %3, align 8
//UL +   %36 = load i64, ptr %4, align 8
//UL +   %37 = shl i64 %35, %36
//UL +   store i64 %37, ptr %3, align 8
//UL +   %38 = load i64, ptr %3, align 8
//UL +   %39 = load i64, ptr %4, align 8
//UL +   %40 = lshr i64 %38, %39
//UL +   store i64 %40, ptr %3, align 8
//UL +   ret void
//UL + }
//UL +
//UL + attributes #0 = { nounwind }
//UL +
//UL + !llvm.module.flags = !{!0}
//UL +
//UL + !0 = !{i32 2, !"Debug Info Version", i32 3}
