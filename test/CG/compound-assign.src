// R      %srcc --ir --short-filenames %s
// R[//L] %srcc --llvm --short-filenames %s
program test;

proc x (int a, in int b) {
    a += b;
    a +~= b;
    a -= b;
    a -~= b;
    a *= b;
    a *~= b;
    a /= b;
    a %= b;
    a <<= b;
    a >>= b;
    a <<<= b;
    a >>>= b;
}

// * @0 = "<<<\00"
// + @1 = "shift amount exceeds bit width\00"
// + @2 = "<<\00"
// + @3 = "%\00"
// + @4 = "division by zero\00"
// + @5 = "/\00"
// + @6 = "*\00"
// + @7 = "-\00"
// + @8 = "compound-assign.src\00"
// + @9 = "integer overflow\00"
// + @10 = "+\00"
// +
// + proc __src_assert_fail (ptr) external fastcc;
// +
// + proc __src_int_arith_error (ptr) external fastcc;
// +
// + proc __src_main external fastcc {
// + entry:
// +     ret
// + }
// +
// + proc _S1xFvixiE (i64 %0, i64 %1) private fastcc {
// +     #0 = 8, align 8
// +     #1 = 64, align 8
// +
// + entry:
// +     store #0, i64 %0, align 8
// +     %2 = load i64, #0, align 8
// +     %3 = sadd ov i64 %2, %1
// +     br %3:1 to bb1 else bb2
// +
// + bb1:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %4 = ptradd #1, 16
// +     store %4, i64 6, align 8
// +     %5 = ptradd #1, 24
// +     store %5, i64 5, align 8
// +     %6 = ptradd #1, 32
// +     store %6, (ptr, i64) (@10, 1), align 8
// +     %7 = ptradd #1, 48
// +     store %7, (ptr, i64) (@9, 16), align 8
// +     abort at loc("compound-assign.src":6:5) arithmetic(ptr #1)
// +
// + bb2:
// +     store #0, i64 %3:0, align 8
// +     %8 = load i64, #0, align 8
// +     %9 = add i64 %8, %1
// +     store #0, i64 %9, align 8
// +     %10 = load i64, #0, align 8
// +     %11 = ssub ov i64 %10, %1
// +     br %11:1 to bb3 else bb4
// +
// + bb3:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %12 = ptradd #1, 16
// +     store %12, i64 8, align 8
// +     %13 = ptradd #1, 24
// +     store %13, i64 5, align 8
// +     %14 = ptradd #1, 32
// +     store %14, (ptr, i64) (@7, 1), align 8
// +     %15 = ptradd #1, 48
// +     store %15, (ptr, i64) (@9, 16), align 8
// +     abort at loc("compound-assign.src":8:5) arithmetic(ptr #1)
// +
// + bb4:
// +     store #0, i64 %11:0, align 8
// +     %16 = load i64, #0, align 8
// +     %17 = sub i64 %16, %1
// +     store #0, i64 %17, align 8
// +     %18 = load i64, #0, align 8
// +     %19 = smul ov i64 %18, %1
// +     br %19:1 to bb5 else bb6
// +
// + bb5:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %20 = ptradd #1, 16
// +     store %20, i64 10, align 8
// +     %21 = ptradd #1, 24
// +     store %21, i64 5, align 8
// +     %22 = ptradd #1, 32
// +     store %22, (ptr, i64) (@6, 1), align 8
// +     %23 = ptradd #1, 48
// +     store %23, (ptr, i64) (@9, 16), align 8
// +     abort at loc("compound-assign.src":10:5) arithmetic(ptr #1)
// +
// + bb6:
// +     store #0, i64 %19:0, align 8
// +     %24 = load i64, #0, align 8
// +     %25 = mul i64 %24, %1
// +     store #0, i64 %25, align 8
// +     %26 = load i64, #0, align 8
// +     %27 = icmp eq i64 %1, 0
// +     br %27 to bb7 else bb8
// +
// + bb7:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %28 = ptradd #1, 16
// +     store %28, i64 12, align 8
// +     %29 = ptradd #1, 24
// +     store %29, i64 5, align 8
// +     %30 = ptradd #1, 32
// +     store %30, (ptr, i64) (@5, 1), align 8
// +     %31 = ptradd #1, 48
// +     store %31, (ptr, i64) (@4, 16), align 8
// +     abort at loc("compound-assign.src":12:5) arithmetic(ptr #1)
// +
// + bb8:
// +     %32 = icmp eq i64 %26, -9223372036854775808
// +     %33 = icmp eq i64 %1, -1
// +     %34 = and i1 %32, %33
// +     br %34 to bb9 else bb10
// +
// + bb9:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %35 = ptradd #1, 16
// +     store %35, i64 12, align 8
// +     %36 = ptradd #1, 24
// +     store %36, i64 5, align 8
// +     %37 = ptradd #1, 32
// +     store %37, (ptr, i64) (@5, 1), align 8
// +     %38 = ptradd #1, 48
// +     store %38, (ptr, i64) (@9, 16), align 8
// +     abort at loc("compound-assign.src":12:5) arithmetic(ptr #1)
// +
// + bb10:
// +     %39 = sdiv i64 %26, %1
// +     store #0, i64 %39, align 8
// +     %40 = load i64, #0, align 8
// +     %41 = icmp eq i64 %1, 0
// +     br %41 to bb11 else bb12
// +
// + bb11:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %42 = ptradd #1, 16
// +     store %42, i64 13, align 8
// +     %43 = ptradd #1, 24
// +     store %43, i64 5, align 8
// +     %44 = ptradd #1, 32
// +     store %44, (ptr, i64) (@3, 1), align 8
// +     %45 = ptradd #1, 48
// +     store %45, (ptr, i64) (@4, 16), align 8
// +     abort at loc("compound-assign.src":13:5) arithmetic(ptr #1)
// +
// + bb12:
// +     %46 = icmp eq i64 %40, -9223372036854775808
// +     %47 = icmp eq i64 %1, -1
// +     %48 = and i1 %46, %47
// +     br %48 to bb13 else bb14
// +
// + bb13:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %49 = ptradd #1, 16
// +     store %49, i64 13, align 8
// +     %50 = ptradd #1, 24
// +     store %50, i64 5, align 8
// +     %51 = ptradd #1, 32
// +     store %51, (ptr, i64) (@3, 1), align 8
// +     %52 = ptradd #1, 48
// +     store %52, (ptr, i64) (@9, 16), align 8
// +     abort at loc("compound-assign.src":13:5) arithmetic(ptr #1)
// +
// + bb14:
// +     %53 = srem i64 %40, %1
// +     store #0, i64 %53, align 8
// +     %54 = load i64, #0, align 8
// +     %55 = icmp uge i64 %1, 64
// +     br %55 to bb15 else bb16
// +
// + bb15:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %56 = ptradd #1, 16
// +     store %56, i64 14, align 8
// +     %57 = ptradd #1, 24
// +     store %57, i64 5, align 8
// +     %58 = ptradd #1, 32
// +     store %58, (ptr, i64) (@2, 2), align 8
// +     %59 = ptradd #1, 48
// +     store %59, (ptr, i64) (@1, 30), align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #1)
// +
// + bb16:
// +     %60 = shl i64 %54, %1
// +     %61 = ashr i64 %54, 63
// +     %62 = ashr i64 %60, 63
// +     %63 = icmp ne i64 %61, %62
// +     br %63 to bb17 else bb18
// +
// + bb17:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %64 = ptradd #1, 16
// +     store %64, i64 14, align 8
// +     %65 = ptradd #1, 24
// +     store %65, i64 5, align 8
// +     %66 = ptradd #1, 32
// +     store %66, (ptr, i64) (@2, 2), align 8
// +     %67 = ptradd #1, 48
// +     store %67, (ptr, i64) (@9, 16), align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #1)
// +
// + bb18:
// +     store #0, i64 %60, align 8
// +     %68 = load i64, #0, align 8
// +     %69 = ashr i64 %68, %1
// +     store #0, i64 %69, align 8
// +     %70 = load i64, #0, align 8
// +     %71 = icmp uge i64 %1, 64
// +     br %71 to bb19 else bb20
// +
// + bb19:
// +     store #1, (ptr, i64) (@8, 19), align 8
// +     %72 = ptradd #1, 16
// +     store %72, i64 16, align 8
// +     %73 = ptradd #1, 24
// +     store %73, i64 5, align 8
// +     %74 = ptradd #1, 32
// +     store %74, (ptr, i64) (@0, 3), align 8
// +     %75 = ptradd #1, 48
// +     store %75, (ptr, i64) (@1, 30), align 8
// +     abort at loc("compound-assign.src":16:5) arithmetic(ptr #1)
// +
// + bb20:
// +     %76 = shl i64 %70, %1
// +     store #0, i64 %76, align 8
// +     %77 = load i64, #0, align 8
// +     %78 = lshr i64 %77, %1
// +     store #0, i64 %78, align 8
// +     ret
// + }

//L * ; ModuleID = 'test'
//L + source_filename = "test"
//L + target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
//L + target triple = "x86_64-unknown-linux-gnu"
//L +
//L + @__srcc_str.10 = private constant [4 x i8] c"<<<\00", align 1
//L + @__srcc_str.9 = private constant [31 x i8] c"shift amount exceeds bit width\00", align 1
//L + @__srcc_str.8 = private constant [3 x i8] c"<<\00", align 1
//L + @__srcc_str.7 = private constant [2 x i8] c"%\00", align 1
//L + @__srcc_str.6 = private constant [17 x i8] c"division by zero\00", align 1
//L + @__srcc_str.5 = private constant [2 x i8] c"/\00", align 1
//L + @__srcc_str.4 = private constant [2 x i8] c"*\00", align 1
//L + @__srcc_str.3 = private constant [2 x i8] c"-\00", align 1
//L + @__srcc_str.2 = private constant [20 x i8] c"compound-assign.src\00", align 1
//L + @__srcc_str.1 = private constant [17 x i8] c"integer overflow\00", align 1
//L + @__srcc_str.0 = private constant [2 x i8] c"+\00", align 1
//L +
//L + declare fastcc void @__src_assert_fail(ptr)
//L +
//L + declare fastcc void @__src_int_arith_error(ptr)
//L +
//L + define fastcc void @__src_main() {
//L +   ret void
//L + }
//L +
//L + define private fastcc void @_S1xFvixiE(i64 %0, i64 %1) {
//L +   %3 = alloca i8, i64 8, align 8
//L +   %4 = alloca i8, i64 64, align 8
//L +   store i64 %0, ptr %3, align 8
//L +   %5 = load i64, ptr %3, align 8
//L +   %6 = call { i64, i1 } @llvm.sadd.with.overflow.i64(i64 %5, i64 %1)
//L +   %7 = extractvalue { i64, i1 } %6, 0
//L +   %8 = extractvalue { i64, i1 } %6, 1
//L +   br i1 %8, label %9, label %14
//L +
//L + 9:                                                ; preds = %2
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %10 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 6, ptr %10, align 8
//L +   %11 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %11, align 8
//L +   %12 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.0, i64 1 }, ptr %12, align 8
//L +   %13 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.1, i64 16 }, ptr %13, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 14:                                               ; preds = %2
//L +   store i64 %7, ptr %3, align 8
//L +   %15 = load i64, ptr %3, align 8
//L +   %16 = add i64 %15, %1
//L +   store i64 %16, ptr %3, align 8
//L +   %17 = load i64, ptr %3, align 8
//L +   %18 = call { i64, i1 } @llvm.ssub.with.overflow.i64(i64 %17, i64 %1)
//L +   %19 = extractvalue { i64, i1 } %18, 0
//L +   %20 = extractvalue { i64, i1 } %18, 1
//L +   br i1 %20, label %21, label %26
//L +
//L + 21:                                               ; preds = %14
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %22 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 8, ptr %22, align 8
//L +   %23 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %23, align 8
//L +   %24 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.3, i64 1 }, ptr %24, align 8
//L +   %25 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.1, i64 16 }, ptr %25, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 26:                                               ; preds = %14
//L +   store i64 %19, ptr %3, align 8
//L +   %27 = load i64, ptr %3, align 8
//L +   %28 = sub i64 %27, %1
//L +   store i64 %28, ptr %3, align 8
//L +   %29 = load i64, ptr %3, align 8
//L +   %30 = call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %29, i64 %1)
//L +   %31 = extractvalue { i64, i1 } %30, 0
//L +   %32 = extractvalue { i64, i1 } %30, 1
//L +   br i1 %32, label %33, label %38
//L +
//L + 33:                                               ; preds = %26
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %34 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 10, ptr %34, align 8
//L +   %35 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %35, align 8
//L +   %36 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.4, i64 1 }, ptr %36, align 8
//L +   %37 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.1, i64 16 }, ptr %37, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 38:                                               ; preds = %26
//L +   store i64 %31, ptr %3, align 8
//L +   %39 = load i64, ptr %3, align 8
//L +   %40 = mul i64 %39, %1
//L +   store i64 %40, ptr %3, align 8
//L +   %41 = load i64, ptr %3, align 8
//L +   %42 = icmp eq i64 %1, 0
//L +   br i1 %42, label %43, label %48
//L +
//L + 43:                                               ; preds = %38
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %44 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 12, ptr %44, align 8
//L +   %45 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %45, align 8
//L +   %46 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.5, i64 1 }, ptr %46, align 8
//L +   %47 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.6, i64 16 }, ptr %47, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 48:                                               ; preds = %38
//L +   %49 = icmp eq i64 %41, -9223372036854775808
//L +   %50 = icmp eq i64 %1, -1
//L +   %51 = and i1 %49, %50
//L +   br i1 %51, label %52, label %57
//L +
//L + 52:                                               ; preds = %48
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %53 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 12, ptr %53, align 8
//L +   %54 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %54, align 8
//L +   %55 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.5, i64 1 }, ptr %55, align 8
//L +   %56 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.1, i64 16 }, ptr %56, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 57:                                               ; preds = %48
//L +   %58 = sdiv i64 %41, %1
//L +   store i64 %58, ptr %3, align 8
//L +   %59 = load i64, ptr %3, align 8
//L +   %60 = icmp eq i64 %1, 0
//L +   br i1 %60, label %61, label %66
//L +
//L + 61:                                               ; preds = %57
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %62 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 13, ptr %62, align 8
//L +   %63 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %63, align 8
//L +   %64 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.7, i64 1 }, ptr %64, align 8
//L +   %65 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.6, i64 16 }, ptr %65, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 66:                                               ; preds = %57
//L +   %67 = icmp eq i64 %59, -9223372036854775808
//L +   %68 = icmp eq i64 %1, -1
//L +   %69 = and i1 %67, %68
//L +   br i1 %69, label %70, label %75
//L +
//L + 70:                                               ; preds = %66
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %71 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 13, ptr %71, align 8
//L +   %72 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %72, align 8
//L +   %73 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.7, i64 1 }, ptr %73, align 8
//L +   %74 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.1, i64 16 }, ptr %74, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 75:                                               ; preds = %66
//L +   %76 = srem i64 %59, %1
//L +   store i64 %76, ptr %3, align 8
//L +   %77 = load i64, ptr %3, align 8
//L +   %78 = icmp uge i64 %1, 64
//L +   br i1 %78, label %79, label %84
//L +
//L + 79:                                               ; preds = %75
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %80 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 14, ptr %80, align 8
//L +   %81 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %81, align 8
//L +   %82 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.8, i64 2 }, ptr %82, align 8
//L +   %83 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.9, i64 30 }, ptr %83, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 84:                                               ; preds = %75
//L +   %85 = shl i64 %77, %1
//L +   %86 = ashr i64 %77, 63
//L +   %87 = ashr i64 %85, 63
//L +   %88 = icmp ne i64 %86, %87
//L +   br i1 %88, label %89, label %94
//L +
//L + 89:                                               ; preds = %84
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %90 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 14, ptr %90, align 8
//L +   %91 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %91, align 8
//L +   %92 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.8, i64 2 }, ptr %92, align 8
//L +   %93 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.1, i64 16 }, ptr %93, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 94:                                               ; preds = %84
//L +   store i64 %85, ptr %3, align 8
//L +   %95 = load i64, ptr %3, align 8
//L +   %96 = ashr i64 %95, %1
//L +   store i64 %96, ptr %3, align 8
//L +   %97 = load i64, ptr %3, align 8
//L +   %98 = icmp uge i64 %1, 64
//L +   br i1 %98, label %99, label %104
//L +
//L + 99:                                               ; preds = %94
//L +   store { ptr, i64 } { ptr @__srcc_str.2, i64 19 }, ptr %4, align 8
//L +   %100 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 16, ptr %100, align 8
//L +   %101 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %101, align 8
//L +   %102 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store { ptr, i64 } { ptr @__srcc_str.10, i64 3 }, ptr %102, align 8
//L +   %103 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store { ptr, i64 } { ptr @__srcc_str.9, i64 30 }, ptr %103, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 104:                                              ; preds = %94
//L +   %105 = shl i64 %97, %1
//L +   store i64 %105, ptr %3, align 8
//L +   %106 = load i64, ptr %3, align 8
//L +   %107 = lshr i64 %106, %1
//L +   store i64 %107, ptr %3, align 8
//L +   ret void
//L + }
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.sadd.with.overflow.i64(i64, i64) #0
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.ssub.with.overflow.i64(i64, i64) #0
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.smul.with.overflow.i64(i64, i64) #0
//L +
//L + attributes #0 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
//L +
//L + !llvm.module.flags = !{!0}
//L +
//L + !0 = !{i32 2, !"Debug Info Version", i32 3}
