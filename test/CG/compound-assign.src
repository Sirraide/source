// R       %srcc --ir --short-filenames --target x86_64-unknown-linux %s
// R[//U]  %srcc --ir -fno-overflow-checks --target x86_64-unknown-linux %s
// R[//L]  %srcc --llvm --short-filenames --target x86_64-unknown-linux %s
// R[//UL] %srcc --llvm -fno-overflow-checks --target x86_64-unknown-linux %s
program test;

proc x (int a, in int b) {
    a += b;
    a +~= b;
    a -= b;
    a -~= b;
    a *= b;
    a *~= b;
    a /= b;
    a %= b;
    a <<= b;
    a >>= b;
    a <<<= b;
    a >>>= b;
}

// * @0 = "<<<\00"
// + @1 = "shift amount exceeds bit width\00"
// + @2 = "<<\00"
// + @3 = "%\00"
// + @4 = "division by zero\00"
// + @5 = "/\00"
// + @6 = "*\00"
// + @7 = "-\00"
// + @8 = "compound-assign.src\00"
// + @9 = "integer overflow\00"
// + @10 = "+\00"
// +
// + proc __src_main external ccc {
// + entry:
// +     ret
// + }
// +
// + proc _S1xFvixiE (i64 %0, i64 %1) private ccc {
// +     #0 = 8, align 8
// +     #1 = 8, align 8
// +     #2 = 80, align 8
// +
// + entry:
// +     store #0, i64 %0, align 8
// +     store #1, i64 %1, align 8
// +     %2 = load i64, #0, align 8
// +     %3 = load i64, #1, align 8
// +     %4 = sadd ov i64 %2, %3
// +     br %4:1 to bb1 else bb2
// +
// + bb1:
// +     store #2, ptr @8, align 8
// +     %5 = ptradd #2, 8
// +     store %5, i64 19, align 8
// +     %6 = ptradd #2, 16
// +     store %6, i64 8, align 8
// +     %7 = ptradd #2, 24
// +     store %7, i64 5, align 8
// +     %8 = ptradd #2, 32
// +     store %8, ptr @10, align 8
// +     %9 = ptradd %8, 8
// +     store %9, i64 1, align 8
// +     %10 = ptradd #2, 48
// +     store %10, ptr @9, align 8
// +     %11 = ptradd %10, 8
// +     store %11, i64 16, align 8
// +     %12 = ptradd #2, 64
// +     store %12, ptr nil, align 8
// +     %13 = ptradd %12, 8
// +     store %13, ptr nil, align 8
// +     abort at loc("compound-assign.src":8:5) arithmetic(ptr #2)
// +
// + bb2:
// +     store #0, i64 %4:0, align 8
// +     %14 = load i64, #0, align 8
// +     %15 = load i64, #1, align 8
// +     %16 = add i64 %14, %15
// +     store #0, i64 %16, align 8
// +     %17 = load i64, #0, align 8
// +     %18 = load i64, #1, align 8
// +     %19 = ssub ov i64 %17, %18
// +     br %19:1 to bb3 else bb4
// +
// + bb3:
// +     store #2, ptr @8, align 8
// +     %20 = ptradd #2, 8
// +     store %20, i64 19, align 8
// +     %21 = ptradd #2, 16
// +     store %21, i64 10, align 8
// +     %22 = ptradd #2, 24
// +     store %22, i64 5, align 8
// +     %23 = ptradd #2, 32
// +     store %23, ptr @7, align 8
// +     %24 = ptradd %23, 8
// +     store %24, i64 1, align 8
// +     %25 = ptradd #2, 48
// +     store %25, ptr @9, align 8
// +     %26 = ptradd %25, 8
// +     store %26, i64 16, align 8
// +     %27 = ptradd #2, 64
// +     store %27, ptr nil, align 8
// +     %28 = ptradd %27, 8
// +     store %28, ptr nil, align 8
// +     abort at loc("compound-assign.src":10:5) arithmetic(ptr #2)
// +
// + bb4:
// +     store #0, i64 %19:0, align 8
// +     %29 = load i64, #0, align 8
// +     %30 = load i64, #1, align 8
// +     %31 = sub i64 %29, %30
// +     store #0, i64 %31, align 8
// +     %32 = load i64, #0, align 8
// +     %33 = load i64, #1, align 8
// +     %34 = smul ov i64 %32, %33
// +     br %34:1 to bb5 else bb6
// +
// + bb5:
// +     store #2, ptr @8, align 8
// +     %35 = ptradd #2, 8
// +     store %35, i64 19, align 8
// +     %36 = ptradd #2, 16
// +     store %36, i64 12, align 8
// +     %37 = ptradd #2, 24
// +     store %37, i64 5, align 8
// +     %38 = ptradd #2, 32
// +     store %38, ptr @6, align 8
// +     %39 = ptradd %38, 8
// +     store %39, i64 1, align 8
// +     %40 = ptradd #2, 48
// +     store %40, ptr @9, align 8
// +     %41 = ptradd %40, 8
// +     store %41, i64 16, align 8
// +     %42 = ptradd #2, 64
// +     store %42, ptr nil, align 8
// +     %43 = ptradd %42, 8
// +     store %43, ptr nil, align 8
// +     abort at loc("compound-assign.src":12:5) arithmetic(ptr #2)
// +
// + bb6:
// +     store #0, i64 %34:0, align 8
// +     %44 = load i64, #0, align 8
// +     %45 = load i64, #1, align 8
// +     %46 = mul i64 %44, %45
// +     store #0, i64 %46, align 8
// +     %47 = load i64, #0, align 8
// +     %48 = load i64, #1, align 8
// +     %49 = icmp eq i64 %48, 0
// +     br %49 to bb7 else bb8
// +
// + bb7:
// +     store #2, ptr @8, align 8
// +     %50 = ptradd #2, 8
// +     store %50, i64 19, align 8
// +     %51 = ptradd #2, 16
// +     store %51, i64 14, align 8
// +     %52 = ptradd #2, 24
// +     store %52, i64 5, align 8
// +     %53 = ptradd #2, 32
// +     store %53, ptr @5, align 8
// +     %54 = ptradd %53, 8
// +     store %54, i64 1, align 8
// +     %55 = ptradd #2, 48
// +     store %55, ptr @4, align 8
// +     %56 = ptradd %55, 8
// +     store %56, i64 16, align 8
// +     %57 = ptradd #2, 64
// +     store %57, ptr nil, align 8
// +     %58 = ptradd %57, 8
// +     store %58, ptr nil, align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #2)
// +
// + bb8:
// +     %59 = icmp eq i64 %47, -9223372036854775808
// +     %60 = icmp eq i64 %48, -1
// +     %61 = and i1 %59, %60
// +     br %61 to bb9 else bb10
// +
// + bb9:
// +     store #2, ptr @8, align 8
// +     %62 = ptradd #2, 8
// +     store %62, i64 19, align 8
// +     %63 = ptradd #2, 16
// +     store %63, i64 14, align 8
// +     %64 = ptradd #2, 24
// +     store %64, i64 5, align 8
// +     %65 = ptradd #2, 32
// +     store %65, ptr @5, align 8
// +     %66 = ptradd %65, 8
// +     store %66, i64 1, align 8
// +     %67 = ptradd #2, 48
// +     store %67, ptr @9, align 8
// +     %68 = ptradd %67, 8
// +     store %68, i64 16, align 8
// +     %69 = ptradd #2, 64
// +     store %69, ptr nil, align 8
// +     %70 = ptradd %69, 8
// +     store %70, ptr nil, align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #2)
// +
// + bb10:
// +     %71 = sdiv i64 %47, %48
// +     store #0, i64 %71, align 8
// +     %72 = load i64, #0, align 8
// +     %73 = load i64, #1, align 8
// +     %74 = icmp eq i64 %73, 0
// +     br %74 to bb11 else bb12
// +
// + bb11:
// +     store #2, ptr @8, align 8
// +     %75 = ptradd #2, 8
// +     store %75, i64 19, align 8
// +     %76 = ptradd #2, 16
// +     store %76, i64 15, align 8
// +     %77 = ptradd #2, 24
// +     store %77, i64 5, align 8
// +     %78 = ptradd #2, 32
// +     store %78, ptr @3, align 8
// +     %79 = ptradd %78, 8
// +     store %79, i64 1, align 8
// +     %80 = ptradd #2, 48
// +     store %80, ptr @4, align 8
// +     %81 = ptradd %80, 8
// +     store %81, i64 16, align 8
// +     %82 = ptradd #2, 64
// +     store %82, ptr nil, align 8
// +     %83 = ptradd %82, 8
// +     store %83, ptr nil, align 8
// +     abort at loc("compound-assign.src":15:5) arithmetic(ptr #2)
// +
// + bb12:
// +     %84 = icmp eq i64 %72, -9223372036854775808
// +     %85 = icmp eq i64 %73, -1
// +     %86 = and i1 %84, %85
// +     br %86 to bb13 else bb14
// +
// + bb13:
// +     store #2, ptr @8, align 8
// +     %87 = ptradd #2, 8
// +     store %87, i64 19, align 8
// +     %88 = ptradd #2, 16
// +     store %88, i64 15, align 8
// +     %89 = ptradd #2, 24
// +     store %89, i64 5, align 8
// +     %90 = ptradd #2, 32
// +     store %90, ptr @3, align 8
// +     %91 = ptradd %90, 8
// +     store %91, i64 1, align 8
// +     %92 = ptradd #2, 48
// +     store %92, ptr @9, align 8
// +     %93 = ptradd %92, 8
// +     store %93, i64 16, align 8
// +     %94 = ptradd #2, 64
// +     store %94, ptr nil, align 8
// +     %95 = ptradd %94, 8
// +     store %95, ptr nil, align 8
// +     abort at loc("compound-assign.src":15:5) arithmetic(ptr #2)
// +
// + bb14:
// +     %96 = srem i64 %72, %73
// +     store #0, i64 %96, align 8
// +     %97 = load i64, #0, align 8
// +     %98 = load i64, #1, align 8
// +     %99 = icmp uge i64 %98, 64
// +     br %99 to bb15 else bb16
// +
// + bb15:
// +     store #2, ptr @8, align 8
// +     %100 = ptradd #2, 8
// +     store %100, i64 19, align 8
// +     %101 = ptradd #2, 16
// +     store %101, i64 16, align 8
// +     %102 = ptradd #2, 24
// +     store %102, i64 5, align 8
// +     %103 = ptradd #2, 32
// +     store %103, ptr @2, align 8
// +     %104 = ptradd %103, 8
// +     store %104, i64 2, align 8
// +     %105 = ptradd #2, 48
// +     store %105, ptr @1, align 8
// +     %106 = ptradd %105, 8
// +     store %106, i64 30, align 8
// +     %107 = ptradd #2, 64
// +     store %107, ptr nil, align 8
// +     %108 = ptradd %107, 8
// +     store %108, ptr nil, align 8
// +     abort at loc("compound-assign.src":16:5) arithmetic(ptr #2)
// +
// + bb16:
// +     %109 = shl i64 %97, %98
// +     %110 = ashr i64 %97, 63
// +     %111 = ashr i64 %109, 63
// +     %112 = icmp ne i64 %110, %111
// +     br %112 to bb17 else bb18
// +
// + bb17:
// +     store #2, ptr @8, align 8
// +     %113 = ptradd #2, 8
// +     store %113, i64 19, align 8
// +     %114 = ptradd #2, 16
// +     store %114, i64 16, align 8
// +     %115 = ptradd #2, 24
// +     store %115, i64 5, align 8
// +     %116 = ptradd #2, 32
// +     store %116, ptr @2, align 8
// +     %117 = ptradd %116, 8
// +     store %117, i64 2, align 8
// +     %118 = ptradd #2, 48
// +     store %118, ptr @9, align 8
// +     %119 = ptradd %118, 8
// +     store %119, i64 16, align 8
// +     %120 = ptradd #2, 64
// +     store %120, ptr nil, align 8
// +     %121 = ptradd %120, 8
// +     store %121, ptr nil, align 8
// +     abort at loc("compound-assign.src":16:5) arithmetic(ptr #2)
// +
// + bb18:
// +     store #0, i64 %109, align 8
// +     %122 = load i64, #0, align 8
// +     %123 = load i64, #1, align 8
// +     %124 = ashr i64 %122, %123
// +     store #0, i64 %124, align 8
// +     %125 = load i64, #0, align 8
// +     %126 = load i64, #1, align 8
// +     %127 = icmp uge i64 %126, 64
// +     br %127 to bb19 else bb20
// +
// + bb19:
// +     store #2, ptr @8, align 8
// +     %128 = ptradd #2, 8
// +     store %128, i64 19, align 8
// +     %129 = ptradd #2, 16
// +     store %129, i64 18, align 8
// +     %130 = ptradd #2, 24
// +     store %130, i64 5, align 8
// +     %131 = ptradd #2, 32
// +     store %131, ptr @0, align 8
// +     %132 = ptradd %131, 8
// +     store %132, i64 3, align 8
// +     %133 = ptradd #2, 48
// +     store %133, ptr @1, align 8
// +     %134 = ptradd %133, 8
// +     store %134, i64 30, align 8
// +     %135 = ptradd #2, 64
// +     store %135, ptr nil, align 8
// +     %136 = ptradd %135, 8
// +     store %136, ptr nil, align 8
// +     abort at loc("compound-assign.src":18:5) arithmetic(ptr #2)
// +
// + bb20:
// +     %137 = shl i64 %125, %126
// +     store #0, i64 %137, align 8
// +     %138 = load i64, #0, align 8
// +     %139 = load i64, #1, align 8
// +     %140 = lshr i64 %138, %139
// +     store #0, i64 %140, align 8
// +     ret
// + }

//U * proc __src_main external ccc {
//U + entry:
//U +     ret
//U + }
//U +
//U + proc _S1xFvixiE (i64 %0, i64 %1) private ccc {
//U +     #0 = 8, align 8
//U +     #1 = 8, align 8
//U +
//U + entry:
//U +     store #0, i64 %0, align 8
//U +     store #1, i64 %1, align 8
//U +     %2 = load i64, #0, align 8
//U +     %3 = load i64, #1, align 8
//U +     %4 = add i64 %2, %3
//U +     store #0, i64 %4, align 8
//U +     %5 = load i64, #0, align 8
//U +     %6 = load i64, #1, align 8
//U +     %7 = add i64 %5, %6
//U +     store #0, i64 %7, align 8
//U +     %8 = load i64, #0, align 8
//U +     %9 = load i64, #1, align 8
//U +     %10 = sub i64 %8, %9
//U +     store #0, i64 %10, align 8
//U +     %11 = load i64, #0, align 8
//U +     %12 = load i64, #1, align 8
//U +     %13 = sub i64 %11, %12
//U +     store #0, i64 %13, align 8
//U +     %14 = load i64, #0, align 8
//U +     %15 = load i64, #1, align 8
//U +     %16 = mul i64 %14, %15
//U +     store #0, i64 %16, align 8
//U +     %17 = load i64, #0, align 8
//U +     %18 = load i64, #1, align 8
//U +     %19 = mul i64 %17, %18
//U +     store #0, i64 %19, align 8
//U +     %20 = load i64, #0, align 8
//U +     %21 = load i64, #1, align 8
//U +     %22 = sdiv i64 %20, %21
//U +     store #0, i64 %22, align 8
//U +     %23 = load i64, #0, align 8
//U +     %24 = load i64, #1, align 8
//U +     %25 = srem i64 %23, %24
//U +     store #0, i64 %25, align 8
//U +     %26 = load i64, #0, align 8
//U +     %27 = load i64, #1, align 8
//U +     %28 = shl i64 %26, %27
//U +     store #0, i64 %28, align 8
//U +     %29 = load i64, #0, align 8
//U +     %30 = load i64, #1, align 8
//U +     %31 = ashr i64 %29, %30
//U +     store #0, i64 %31, align 8
//U +     %32 = load i64, #0, align 8
//U +     %33 = load i64, #1, align 8
//U +     %34 = shl i64 %32, %33
//U +     store #0, i64 %34, align 8
//U +     %35 = load i64, #0, align 8
//U +     %36 = load i64, #1, align 8
//U +     %37 = lshr i64 %35, %36
//U +     store #0, i64 %37, align 8
//U +     ret
//U + }

//L * ; ModuleID = 'test'
//L + source_filename = "test"
//L + target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
//L + target triple = "x86_64-unknown-linux-gnu"
//L +
//L + @__srcc_str.10 = private constant [4 x i8] c"<<<\00", align 1
//L + @__srcc_str.9 = private constant [31 x i8] c"shift amount exceeds bit width\00", align 1
//L + @__srcc_str.8 = private constant [3 x i8] c"<<\00", align 1
//L + @__srcc_str.7 = private constant [2 x i8] c"%\00", align 1
//L + @__srcc_str.6 = private constant [17 x i8] c"division by zero\00", align 1
//L + @__srcc_str.5 = private constant [2 x i8] c"/\00", align 1
//L + @__srcc_str.4 = private constant [2 x i8] c"*\00", align 1
//L + @__srcc_str.3 = private constant [2 x i8] c"-\00", align 1
//L + @__srcc_str.2 = private constant [20 x i8] c"compound-assign.src\00", align 1
//L + @__srcc_str.1 = private constant [17 x i8] c"integer overflow\00", align 1
//L + @__srcc_str.0 = private constant [2 x i8] c"+\00", align 1
//L +
//L + ; Function Attrs: nounwind
//L + define void @__src_main() #0 {
//L +   ret void
//L + }
//L +
//L + ; Function Attrs: nounwind
//L + define private void @_S1xFvixiE(i64 %0, i64 %1) #0 {
//L +   %3 = alloca i8, i64 8, align 8
//L +   %4 = alloca i8, i64 8, align 8
//L +   %5 = alloca i8, i64 80, align 8
//L +   store i64 %0, ptr %3, align 8
//L +   store i64 %1, ptr %4, align 8
//L +   %6 = load i64, ptr %3, align 8
//L +   %7 = load i64, ptr %4, align 8
//L +   %8 = call { i64, i1 } @llvm.sadd.with.overflow.i64(i64 %6, i64 %7)
//L +   %9 = extractvalue { i64, i1 } %8, 0
//L +   %10 = extractvalue { i64, i1 } %8, 1
//L +   br i1 %10, label %11, label %21
//L +
//L + 11:                                               ; preds = %2
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %12 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %12, align 8
//L +   %13 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 8, ptr %13, align 8
//L +   %14 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %14, align 8
//L +   %15 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.0, ptr %15, align 8
//L +   %16 = getelementptr inbounds nuw i8, ptr %15, i32 8
//L +   store i64 1, ptr %16, align 8
//L +   %17 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %17, align 8
//L +   %18 = getelementptr inbounds nuw i8, ptr %17, i32 8
//L +   store i64 16, ptr %18, align 8
//L +   %19 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %19, align 8
//L +   %20 = getelementptr inbounds nuw i8, ptr %19, i32 8
//L +   store ptr null, ptr %20, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 21:                                               ; preds = %2
//L +   store i64 %9, ptr %3, align 8
//L +   %22 = load i64, ptr %3, align 8
//L +   %23 = load i64, ptr %4, align 8
//L +   %24 = add i64 %22, %23
//L +   store i64 %24, ptr %3, align 8
//L +   %25 = load i64, ptr %3, align 8
//L +   %26 = load i64, ptr %4, align 8
//L +   %27 = call { i64, i1 } @llvm.ssub.with.overflow.i64(i64 %25, i64 %26)
//L +   %28 = extractvalue { i64, i1 } %27, 0
//L +   %29 = extractvalue { i64, i1 } %27, 1
//L +   br i1 %29, label %30, label %40
//L +
//L + 30:                                               ; preds = %21
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %31 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %31, align 8
//L +   %32 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 10, ptr %32, align 8
//L +   %33 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %33, align 8
//L +   %34 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.3, ptr %34, align 8
//L +   %35 = getelementptr inbounds nuw i8, ptr %34, i32 8
//L +   store i64 1, ptr %35, align 8
//L +   %36 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %36, align 8
//L +   %37 = getelementptr inbounds nuw i8, ptr %36, i32 8
//L +   store i64 16, ptr %37, align 8
//L +   %38 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %38, align 8
//L +   %39 = getelementptr inbounds nuw i8, ptr %38, i32 8
//L +   store ptr null, ptr %39, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 40:                                               ; preds = %21
//L +   store i64 %28, ptr %3, align 8
//L +   %41 = load i64, ptr %3, align 8
//L +   %42 = load i64, ptr %4, align 8
//L +   %43 = sub i64 %41, %42
//L +   store i64 %43, ptr %3, align 8
//L +   %44 = load i64, ptr %3, align 8
//L +   %45 = load i64, ptr %4, align 8
//L +   %46 = call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %44, i64 %45)
//L +   %47 = extractvalue { i64, i1 } %46, 0
//L +   %48 = extractvalue { i64, i1 } %46, 1
//L +   br i1 %48, label %49, label %59
//L +
//L + 49:                                               ; preds = %40
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %50 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %50, align 8
//L +   %51 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 12, ptr %51, align 8
//L +   %52 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %52, align 8
//L +   %53 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.4, ptr %53, align 8
//L +   %54 = getelementptr inbounds nuw i8, ptr %53, i32 8
//L +   store i64 1, ptr %54, align 8
//L +   %55 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %55, align 8
//L +   %56 = getelementptr inbounds nuw i8, ptr %55, i32 8
//L +   store i64 16, ptr %56, align 8
//L +   %57 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %57, align 8
//L +   %58 = getelementptr inbounds nuw i8, ptr %57, i32 8
//L +   store ptr null, ptr %58, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 59:                                               ; preds = %40
//L +   store i64 %47, ptr %3, align 8
//L +   %60 = load i64, ptr %3, align 8
//L +   %61 = load i64, ptr %4, align 8
//L +   %62 = mul i64 %60, %61
//L +   store i64 %62, ptr %3, align 8
//L +   %63 = load i64, ptr %3, align 8
//L +   %64 = load i64, ptr %4, align 8
//L +   %65 = icmp eq i64 %64, 0
//L +   br i1 %65, label %66, label %76
//L +
//L + 66:                                               ; preds = %59
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %67 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %67, align 8
//L +   %68 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 14, ptr %68, align 8
//L +   %69 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %69, align 8
//L +   %70 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.5, ptr %70, align 8
//L +   %71 = getelementptr inbounds nuw i8, ptr %70, i32 8
//L +   store i64 1, ptr %71, align 8
//L +   %72 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.6, ptr %72, align 8
//L +   %73 = getelementptr inbounds nuw i8, ptr %72, i32 8
//L +   store i64 16, ptr %73, align 8
//L +   %74 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %74, align 8
//L +   %75 = getelementptr inbounds nuw i8, ptr %74, i32 8
//L +   store ptr null, ptr %75, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 76:                                               ; preds = %59
//L +   %77 = icmp eq i64 %63, -9223372036854775808
//L +   %78 = icmp eq i64 %64, -1
//L +   %79 = and i1 %77, %78
//L +   br i1 %79, label %80, label %90
//L +
//L + 80:                                               ; preds = %76
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %81 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %81, align 8
//L +   %82 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 14, ptr %82, align 8
//L +   %83 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %83, align 8
//L +   %84 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.5, ptr %84, align 8
//L +   %85 = getelementptr inbounds nuw i8, ptr %84, i32 8
//L +   store i64 1, ptr %85, align 8
//L +   %86 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %86, align 8
//L +   %87 = getelementptr inbounds nuw i8, ptr %86, i32 8
//L +   store i64 16, ptr %87, align 8
//L +   %88 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %88, align 8
//L +   %89 = getelementptr inbounds nuw i8, ptr %88, i32 8
//L +   store ptr null, ptr %89, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 90:                                               ; preds = %76
//L +   %91 = sdiv i64 %63, %64
//L +   store i64 %91, ptr %3, align 8
//L +   %92 = load i64, ptr %3, align 8
//L +   %93 = load i64, ptr %4, align 8
//L +   %94 = icmp eq i64 %93, 0
//L +   br i1 %94, label %95, label %105
//L +
//L + 95:                                               ; preds = %90
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %96 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %96, align 8
//L +   %97 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 15, ptr %97, align 8
//L +   %98 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %98, align 8
//L +   %99 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.7, ptr %99, align 8
//L +   %100 = getelementptr inbounds nuw i8, ptr %99, i32 8
//L +   store i64 1, ptr %100, align 8
//L +   %101 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.6, ptr %101, align 8
//L +   %102 = getelementptr inbounds nuw i8, ptr %101, i32 8
//L +   store i64 16, ptr %102, align 8
//L +   %103 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %103, align 8
//L +   %104 = getelementptr inbounds nuw i8, ptr %103, i32 8
//L +   store ptr null, ptr %104, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 105:                                              ; preds = %90
//L +   %106 = icmp eq i64 %92, -9223372036854775808
//L +   %107 = icmp eq i64 %93, -1
//L +   %108 = and i1 %106, %107
//L +   br i1 %108, label %109, label %119
//L +
//L + 109:                                              ; preds = %105
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %110 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %110, align 8
//L +   %111 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 15, ptr %111, align 8
//L +   %112 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %112, align 8
//L +   %113 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.7, ptr %113, align 8
//L +   %114 = getelementptr inbounds nuw i8, ptr %113, i32 8
//L +   store i64 1, ptr %114, align 8
//L +   %115 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %115, align 8
//L +   %116 = getelementptr inbounds nuw i8, ptr %115, i32 8
//L +   store i64 16, ptr %116, align 8
//L +   %117 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %117, align 8
//L +   %118 = getelementptr inbounds nuw i8, ptr %117, i32 8
//L +   store ptr null, ptr %118, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 119:                                              ; preds = %105
//L +   %120 = srem i64 %92, %93
//L +   store i64 %120, ptr %3, align 8
//L +   %121 = load i64, ptr %3, align 8
//L +   %122 = load i64, ptr %4, align 8
//L +   %123 = icmp uge i64 %122, 64
//L +   br i1 %123, label %124, label %134
//L +
//L + 124:                                              ; preds = %119
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %125 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %125, align 8
//L +   %126 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 16, ptr %126, align 8
//L +   %127 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %127, align 8
//L +   %128 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.8, ptr %128, align 8
//L +   %129 = getelementptr inbounds nuw i8, ptr %128, i32 8
//L +   store i64 2, ptr %129, align 8
//L +   %130 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.9, ptr %130, align 8
//L +   %131 = getelementptr inbounds nuw i8, ptr %130, i32 8
//L +   store i64 30, ptr %131, align 8
//L +   %132 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %132, align 8
//L +   %133 = getelementptr inbounds nuw i8, ptr %132, i32 8
//L +   store ptr null, ptr %133, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 134:                                              ; preds = %119
//L +   %135 = shl i64 %121, %122
//L +   %136 = ashr i64 %121, 63
//L +   %137 = ashr i64 %135, 63
//L +   %138 = icmp ne i64 %136, %137
//L +   br i1 %138, label %139, label %149
//L +
//L + 139:                                              ; preds = %134
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %140 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %140, align 8
//L +   %141 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 16, ptr %141, align 8
//L +   %142 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %142, align 8
//L +   %143 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.8, ptr %143, align 8
//L +   %144 = getelementptr inbounds nuw i8, ptr %143, i32 8
//L +   store i64 2, ptr %144, align 8
//L +   %145 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.1, ptr %145, align 8
//L +   %146 = getelementptr inbounds nuw i8, ptr %145, i32 8
//L +   store i64 16, ptr %146, align 8
//L +   %147 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %147, align 8
//L +   %148 = getelementptr inbounds nuw i8, ptr %147, i32 8
//L +   store ptr null, ptr %148, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 149:                                              ; preds = %134
//L +   store i64 %135, ptr %3, align 8
//L +   %150 = load i64, ptr %3, align 8
//L +   %151 = load i64, ptr %4, align 8
//L +   %152 = ashr i64 %150, %151
//L +   store i64 %152, ptr %3, align 8
//L +   %153 = load i64, ptr %3, align 8
//L +   %154 = load i64, ptr %4, align 8
//L +   %155 = icmp uge i64 %154, 64
//L +   br i1 %155, label %156, label %166
//L +
//L + 156:                                              ; preds = %149
//L +   store ptr @__srcc_str.2, ptr %5, align 8
//L +   %157 = getelementptr inbounds nuw i8, ptr %5, i32 8
//L +   store i64 19, ptr %157, align 8
//L +   %158 = getelementptr inbounds nuw i8, ptr %5, i32 16
//L +   store i64 18, ptr %158, align 8
//L +   %159 = getelementptr inbounds nuw i8, ptr %5, i32 24
//L +   store i64 5, ptr %159, align 8
//L +   %160 = getelementptr inbounds nuw i8, ptr %5, i32 32
//L +   store ptr @__srcc_str.10, ptr %160, align 8
//L +   %161 = getelementptr inbounds nuw i8, ptr %160, i32 8
//L +   store i64 3, ptr %161, align 8
//L +   %162 = getelementptr inbounds nuw i8, ptr %5, i32 48
//L +   store ptr @__srcc_str.9, ptr %162, align 8
//L +   %163 = getelementptr inbounds nuw i8, ptr %162, i32 8
//L +   store i64 30, ptr %163, align 8
//L +   %164 = getelementptr inbounds nuw i8, ptr %5, i32 64
//L +   store ptr null, ptr %164, align 8
//L +   %165 = getelementptr inbounds nuw i8, ptr %164, i32 8
//L +   store ptr null, ptr %165, align 8
//L +   call void @__src_int_arith_error(ptr %5)
//L +   unreachable
//L +
//L + 166:                                              ; preds = %149
//L +   %167 = shl i64 %153, %154
//L +   store i64 %167, ptr %3, align 8
//L +   %168 = load i64, ptr %3, align 8
//L +   %169 = load i64, ptr %4, align 8
//L +   %170 = lshr i64 %168, %169
//L +   store i64 %170, ptr %3, align 8
//L +   ret void
//L + }
//L +
//L + declare void @__src_int_arith_error(ptr)
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.sadd.with.overflow.i64(i64, i64) #1
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.ssub.with.overflow.i64(i64, i64) #1
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.smul.with.overflow.i64(i64, i64) #1
//L +
//L + attributes #0 = { nounwind }
//L + attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
//L +
//L + !llvm.module.flags = !{!0}
//L +
//L + !0 = !{i32 2, !"Debug Info Version", i32 3}

//UL * ; ModuleID = 'test'
//UL + source_filename = "test"
//UL + target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
//UL + target triple = "x86_64-unknown-linux-gnu"
//UL +
//UL + ; Function Attrs: nounwind
//UL + define void @__src_main() #0 {
//UL +   ret void
//UL + }
//UL +
//UL + ; Function Attrs: nounwind
//UL + define private void @_S1xFvixiE(i64 %0, i64 %1) #0 {
//UL +   %3 = alloca i8, i64 8, align 8
//UL +   %4 = alloca i8, i64 8, align 8
//UL +   store i64 %0, ptr %3, align 8
//UL +   store i64 %1, ptr %4, align 8
//UL +   %5 = load i64, ptr %3, align 8
//UL +   %6 = load i64, ptr %4, align 8
//UL +   %7 = add i64 %5, %6
//UL +   store i64 %7, ptr %3, align 8
//UL +   %8 = load i64, ptr %3, align 8
//UL +   %9 = load i64, ptr %4, align 8
//UL +   %10 = add i64 %8, %9
//UL +   store i64 %10, ptr %3, align 8
//UL +   %11 = load i64, ptr %3, align 8
//UL +   %12 = load i64, ptr %4, align 8
//UL +   %13 = sub i64 %11, %12
//UL +   store i64 %13, ptr %3, align 8
//UL +   %14 = load i64, ptr %3, align 8
//UL +   %15 = load i64, ptr %4, align 8
//UL +   %16 = sub i64 %14, %15
//UL +   store i64 %16, ptr %3, align 8
//UL +   %17 = load i64, ptr %3, align 8
//UL +   %18 = load i64, ptr %4, align 8
//UL +   %19 = mul i64 %17, %18
//UL +   store i64 %19, ptr %3, align 8
//UL +   %20 = load i64, ptr %3, align 8
//UL +   %21 = load i64, ptr %4, align 8
//UL +   %22 = mul i64 %20, %21
//UL +   store i64 %22, ptr %3, align 8
//UL +   %23 = load i64, ptr %3, align 8
//UL +   %24 = load i64, ptr %4, align 8
//UL +   %25 = sdiv i64 %23, %24
//UL +   store i64 %25, ptr %3, align 8
//UL +   %26 = load i64, ptr %3, align 8
//UL +   %27 = load i64, ptr %4, align 8
//UL +   %28 = srem i64 %26, %27
//UL +   store i64 %28, ptr %3, align 8
//UL +   %29 = load i64, ptr %3, align 8
//UL +   %30 = load i64, ptr %4, align 8
//UL +   %31 = shl i64 %29, %30
//UL +   store i64 %31, ptr %3, align 8
//UL +   %32 = load i64, ptr %3, align 8
//UL +   %33 = load i64, ptr %4, align 8
//UL +   %34 = ashr i64 %32, %33
//UL +   store i64 %34, ptr %3, align 8
//UL +   %35 = load i64, ptr %3, align 8
//UL +   %36 = load i64, ptr %4, align 8
//UL +   %37 = shl i64 %35, %36
//UL +   store i64 %37, ptr %3, align 8
//UL +   %38 = load i64, ptr %3, align 8
//UL +   %39 = load i64, ptr %4, align 8
//UL +   %40 = lshr i64 %38, %39
//UL +   store i64 %40, ptr %3, align 8
//UL +   ret void
//UL + }
//UL +
//UL + attributes #0 = { nounwind }
//UL +
//UL + !llvm.module.flags = !{!0}
//UL +
//UL + !0 = !{i32 2, !"Debug Info Version", i32 3}
