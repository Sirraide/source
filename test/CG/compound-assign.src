// R      %srcc --ir --short-filenames %s
// R[//L] %srcc --llvm --short-filenames %s
program test;

proc x (int a, in int b) {
    a += b;
    a +~= b;
    a -= b;
    a -~= b;
    a *= b;
    a *~= b;
    a /= b;
    a %= b;
    a <<= b;
    a >>= b;
    a <<<= b;
    a >>>= b;
}

// * @0 = "<<<\00"
// + @1 = "shift amount exceeds bit width\00"
// + @2 = "<<\00"
// + @3 = "%\00"
// + @4 = "division by zero\00"
// + @5 = "/\00"
// + @6 = "*\00"
// + @7 = "-\00"
// + @8 = "compound-assign.src\00"
// + @9 = "integer overflow\00"
// + @10 = "+\00"
// +
// + proc __src_assert_fail (ptr) external fastcc;
// +
// + proc __src_int_arith_error (ptr) external fastcc;
// +
// + proc __src_main external fastcc {
// + entry:
// +     ret
// + }
// +
// + proc _S1xFvixiE (i64 %0, i64 %1) private fastcc {
// +     #0 = 8, align 8
// +     #1 = 64, align 8
// +
// + entry:
// +     store #0, i64 %0, align 8
// +     %2 = load i64, #0, align 8
// +     %3 = sadd ov i64 %2, %1
// +     br %3:1 to bb1 else bb2
// +
// + bb1:
// +     store #1, ptr @8, align 8
// +     %4 = ptradd #1, 8
// +     store %4, i64 19, align 8
// +     %5 = ptradd #1, 16
// +     store %5, i64 6, align 8
// +     %6 = ptradd #1, 24
// +     store %6, i64 5, align 8
// +     %7 = ptradd #1, 32
// +     store %7, ptr @10, align 8
// +     %8 = ptradd %7, 8
// +     store %8, i64 1, align 8
// +     %9 = ptradd #1, 48
// +     store %9, ptr @9, align 8
// +     %10 = ptradd %9, 8
// +     store %10, i64 16, align 8
// +     abort at loc("compound-assign.src":6:5) arithmetic(ptr #1)
// +
// + bb2:
// +     store #0, i64 %3:0, align 8
// +     %11 = load i64, #0, align 8
// +     %12 = add i64 %11, %1
// +     store #0, i64 %12, align 8
// +     %13 = load i64, #0, align 8
// +     %14 = ssub ov i64 %13, %1
// +     br %14:1 to bb3 else bb4
// +
// + bb3:
// +     store #1, ptr @8, align 8
// +     %15 = ptradd #1, 8
// +     store %15, i64 19, align 8
// +     %16 = ptradd #1, 16
// +     store %16, i64 8, align 8
// +     %17 = ptradd #1, 24
// +     store %17, i64 5, align 8
// +     %18 = ptradd #1, 32
// +     store %18, ptr @7, align 8
// +     %19 = ptradd %18, 8
// +     store %19, i64 1, align 8
// +     %20 = ptradd #1, 48
// +     store %20, ptr @9, align 8
// +     %21 = ptradd %20, 8
// +     store %21, i64 16, align 8
// +     abort at loc("compound-assign.src":8:5) arithmetic(ptr #1)
// +
// + bb4:
// +     store #0, i64 %14:0, align 8
// +     %22 = load i64, #0, align 8
// +     %23 = sub i64 %22, %1
// +     store #0, i64 %23, align 8
// +     %24 = load i64, #0, align 8
// +     %25 = smul ov i64 %24, %1
// +     br %25:1 to bb5 else bb6
// +
// + bb5:
// +     store #1, ptr @8, align 8
// +     %26 = ptradd #1, 8
// +     store %26, i64 19, align 8
// +     %27 = ptradd #1, 16
// +     store %27, i64 10, align 8
// +     %28 = ptradd #1, 24
// +     store %28, i64 5, align 8
// +     %29 = ptradd #1, 32
// +     store %29, ptr @6, align 8
// +     %30 = ptradd %29, 8
// +     store %30, i64 1, align 8
// +     %31 = ptradd #1, 48
// +     store %31, ptr @9, align 8
// +     %32 = ptradd %31, 8
// +     store %32, i64 16, align 8
// +     abort at loc("compound-assign.src":10:5) arithmetic(ptr #1)
// +
// + bb6:
// +     store #0, i64 %25:0, align 8
// +     %33 = load i64, #0, align 8
// +     %34 = mul i64 %33, %1
// +     store #0, i64 %34, align 8
// +     %35 = load i64, #0, align 8
// +     %36 = icmp eq i64 %1, 0
// +     br %36 to bb7 else bb8
// +
// + bb7:
// +     store #1, ptr @8, align 8
// +     %37 = ptradd #1, 8
// +     store %37, i64 19, align 8
// +     %38 = ptradd #1, 16
// +     store %38, i64 12, align 8
// +     %39 = ptradd #1, 24
// +     store %39, i64 5, align 8
// +     %40 = ptradd #1, 32
// +     store %40, ptr @5, align 8
// +     %41 = ptradd %40, 8
// +     store %41, i64 1, align 8
// +     %42 = ptradd #1, 48
// +     store %42, ptr @4, align 8
// +     %43 = ptradd %42, 8
// +     store %43, i64 16, align 8
// +     abort at loc("compound-assign.src":12:5) arithmetic(ptr #1)
// +
// + bb8:
// +     %44 = icmp eq i64 %35, -9223372036854775808
// +     %45 = icmp eq i64 %1, -1
// +     %46 = and i1 %44, %45
// +     br %46 to bb9 else bb10
// +
// + bb9:
// +     store #1, ptr @8, align 8
// +     %47 = ptradd #1, 8
// +     store %47, i64 19, align 8
// +     %48 = ptradd #1, 16
// +     store %48, i64 12, align 8
// +     %49 = ptradd #1, 24
// +     store %49, i64 5, align 8
// +     %50 = ptradd #1, 32
// +     store %50, ptr @5, align 8
// +     %51 = ptradd %50, 8
// +     store %51, i64 1, align 8
// +     %52 = ptradd #1, 48
// +     store %52, ptr @9, align 8
// +     %53 = ptradd %52, 8
// +     store %53, i64 16, align 8
// +     abort at loc("compound-assign.src":12:5) arithmetic(ptr #1)
// +
// + bb10:
// +     %54 = sdiv i64 %35, %1
// +     store #0, i64 %54, align 8
// +     %55 = load i64, #0, align 8
// +     %56 = icmp eq i64 %1, 0
// +     br %56 to bb11 else bb12
// +
// + bb11:
// +     store #1, ptr @8, align 8
// +     %57 = ptradd #1, 8
// +     store %57, i64 19, align 8
// +     %58 = ptradd #1, 16
// +     store %58, i64 13, align 8
// +     %59 = ptradd #1, 24
// +     store %59, i64 5, align 8
// +     %60 = ptradd #1, 32
// +     store %60, ptr @3, align 8
// +     %61 = ptradd %60, 8
// +     store %61, i64 1, align 8
// +     %62 = ptradd #1, 48
// +     store %62, ptr @4, align 8
// +     %63 = ptradd %62, 8
// +     store %63, i64 16, align 8
// +     abort at loc("compound-assign.src":13:5) arithmetic(ptr #1)
// +
// + bb12:
// +     %64 = icmp eq i64 %55, -9223372036854775808
// +     %65 = icmp eq i64 %1, -1
// +     %66 = and i1 %64, %65
// +     br %66 to bb13 else bb14
// +
// + bb13:
// +     store #1, ptr @8, align 8
// +     %67 = ptradd #1, 8
// +     store %67, i64 19, align 8
// +     %68 = ptradd #1, 16
// +     store %68, i64 13, align 8
// +     %69 = ptradd #1, 24
// +     store %69, i64 5, align 8
// +     %70 = ptradd #1, 32
// +     store %70, ptr @3, align 8
// +     %71 = ptradd %70, 8
// +     store %71, i64 1, align 8
// +     %72 = ptradd #1, 48
// +     store %72, ptr @9, align 8
// +     %73 = ptradd %72, 8
// +     store %73, i64 16, align 8
// +     abort at loc("compound-assign.src":13:5) arithmetic(ptr #1)
// +
// + bb14:
// +     %74 = srem i64 %55, %1
// +     store #0, i64 %74, align 8
// +     %75 = load i64, #0, align 8
// +     %76 = icmp uge i64 %1, 64
// +     br %76 to bb15 else bb16
// +
// + bb15:
// +     store #1, ptr @8, align 8
// +     %77 = ptradd #1, 8
// +     store %77, i64 19, align 8
// +     %78 = ptradd #1, 16
// +     store %78, i64 14, align 8
// +     %79 = ptradd #1, 24
// +     store %79, i64 5, align 8
// +     %80 = ptradd #1, 32
// +     store %80, ptr @2, align 8
// +     %81 = ptradd %80, 8
// +     store %81, i64 2, align 8
// +     %82 = ptradd #1, 48
// +     store %82, ptr @1, align 8
// +     %83 = ptradd %82, 8
// +     store %83, i64 30, align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #1)
// +
// + bb16:
// +     %84 = shl i64 %75, %1
// +     %85 = ashr i64 %75, 63
// +     %86 = ashr i64 %84, 63
// +     %87 = icmp ne i64 %85, %86
// +     br %87 to bb17 else bb18
// +
// + bb17:
// +     store #1, ptr @8, align 8
// +     %88 = ptradd #1, 8
// +     store %88, i64 19, align 8
// +     %89 = ptradd #1, 16
// +     store %89, i64 14, align 8
// +     %90 = ptradd #1, 24
// +     store %90, i64 5, align 8
// +     %91 = ptradd #1, 32
// +     store %91, ptr @2, align 8
// +     %92 = ptradd %91, 8
// +     store %92, i64 2, align 8
// +     %93 = ptradd #1, 48
// +     store %93, ptr @9, align 8
// +     %94 = ptradd %93, 8
// +     store %94, i64 16, align 8
// +     abort at loc("compound-assign.src":14:5) arithmetic(ptr #1)
// +
// + bb18:
// +     store #0, i64 %84, align 8
// +     %95 = load i64, #0, align 8
// +     %96 = ashr i64 %95, %1
// +     store #0, i64 %96, align 8
// +     %97 = load i64, #0, align 8
// +     %98 = icmp uge i64 %1, 64
// +     br %98 to bb19 else bb20
// +
// + bb19:
// +     store #1, ptr @8, align 8
// +     %99 = ptradd #1, 8
// +     store %99, i64 19, align 8
// +     %100 = ptradd #1, 16
// +     store %100, i64 16, align 8
// +     %101 = ptradd #1, 24
// +     store %101, i64 5, align 8
// +     %102 = ptradd #1, 32
// +     store %102, ptr @0, align 8
// +     %103 = ptradd %102, 8
// +     store %103, i64 3, align 8
// +     %104 = ptradd #1, 48
// +     store %104, ptr @1, align 8
// +     %105 = ptradd %104, 8
// +     store %105, i64 30, align 8
// +     abort at loc("compound-assign.src":16:5) arithmetic(ptr #1)
// +
// + bb20:
// +     %106 = shl i64 %97, %1
// +     store #0, i64 %106, align 8
// +     %107 = load i64, #0, align 8
// +     %108 = lshr i64 %107, %1
// +     store #0, i64 %108, align 8
// +     ret
// + }

//L * ; ModuleID = 'test'
//L + source_filename = "test"
//L + target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128"
//L + target triple = "x86_64-unknown-linux-gnu"
//L +
//L + @__srcc_str.10 = private constant [4 x i8] c"<<<\00", align 1
//L + @__srcc_str.9 = private constant [31 x i8] c"shift amount exceeds bit width\00", align 1
//L + @__srcc_str.8 = private constant [3 x i8] c"<<\00", align 1
//L + @__srcc_str.7 = private constant [2 x i8] c"%\00", align 1
//L + @__srcc_str.6 = private constant [17 x i8] c"division by zero\00", align 1
//L + @__srcc_str.5 = private constant [2 x i8] c"/\00", align 1
//L + @__srcc_str.4 = private constant [2 x i8] c"*\00", align 1
//L + @__srcc_str.3 = private constant [2 x i8] c"-\00", align 1
//L + @__srcc_str.2 = private constant [20 x i8] c"compound-assign.src\00", align 1
//L + @__srcc_str.1 = private constant [17 x i8] c"integer overflow\00", align 1
//L + @__srcc_str.0 = private constant [2 x i8] c"+\00", align 1
//L +
//L + declare fastcc void @__src_assert_fail(ptr)
//L +
//L + declare fastcc void @__src_int_arith_error(ptr)
//L +
//L + define fastcc void @__src_main() {
//L +   ret void
//L + }
//L +
//L + define private fastcc void @_S1xFvixiE(i64 %0, i64 %1) {
//L +   %3 = alloca i8, i64 8, align 8
//L +   %4 = alloca i8, i64 64, align 8
//L +   store i64 %0, ptr %3, align 8
//L +   %5 = load i64, ptr %3, align 8
//L +   %6 = call { i64, i1 } @llvm.sadd.with.overflow.i64(i64 %5, i64 %1)
//L +   %7 = extractvalue { i64, i1 } %6, 0
//L +   %8 = extractvalue { i64, i1 } %6, 1
//L +   br i1 %8, label %9, label %17
//L +
//L + 9:                                                ; preds = %2
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %10 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %10, align 8
//L +   %11 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 6, ptr %11, align 8
//L +   %12 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %12, align 8
//L +   %13 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.0, ptr %13, align 8
//L +   %14 = getelementptr inbounds nuw i8, ptr %13, i32 8
//L +   store i64 1, ptr %14, align 8
//L +   %15 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.1, ptr %15, align 8
//L +   %16 = getelementptr inbounds nuw i8, ptr %15, i32 8
//L +   store i64 16, ptr %16, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 17:                                               ; preds = %2
//L +   store i64 %7, ptr %3, align 8
//L +   %18 = load i64, ptr %3, align 8
//L +   %19 = add i64 %18, %1
//L +   store i64 %19, ptr %3, align 8
//L +   %20 = load i64, ptr %3, align 8
//L +   %21 = call { i64, i1 } @llvm.ssub.with.overflow.i64(i64 %20, i64 %1)
//L +   %22 = extractvalue { i64, i1 } %21, 0
//L +   %23 = extractvalue { i64, i1 } %21, 1
//L +   br i1 %23, label %24, label %32
//L +
//L + 24:                                               ; preds = %17
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %25 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %25, align 8
//L +   %26 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 8, ptr %26, align 8
//L +   %27 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %27, align 8
//L +   %28 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.3, ptr %28, align 8
//L +   %29 = getelementptr inbounds nuw i8, ptr %28, i32 8
//L +   store i64 1, ptr %29, align 8
//L +   %30 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.1, ptr %30, align 8
//L +   %31 = getelementptr inbounds nuw i8, ptr %30, i32 8
//L +   store i64 16, ptr %31, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 32:                                               ; preds = %17
//L +   store i64 %22, ptr %3, align 8
//L +   %33 = load i64, ptr %3, align 8
//L +   %34 = sub i64 %33, %1
//L +   store i64 %34, ptr %3, align 8
//L +   %35 = load i64, ptr %3, align 8
//L +   %36 = call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %35, i64 %1)
//L +   %37 = extractvalue { i64, i1 } %36, 0
//L +   %38 = extractvalue { i64, i1 } %36, 1
//L +   br i1 %38, label %39, label %47
//L +
//L + 39:                                               ; preds = %32
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %40 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %40, align 8
//L +   %41 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 10, ptr %41, align 8
//L +   %42 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %42, align 8
//L +   %43 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.4, ptr %43, align 8
//L +   %44 = getelementptr inbounds nuw i8, ptr %43, i32 8
//L +   store i64 1, ptr %44, align 8
//L +   %45 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.1, ptr %45, align 8
//L +   %46 = getelementptr inbounds nuw i8, ptr %45, i32 8
//L +   store i64 16, ptr %46, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 47:                                               ; preds = %32
//L +   store i64 %37, ptr %3, align 8
//L +   %48 = load i64, ptr %3, align 8
//L +   %49 = mul i64 %48, %1
//L +   store i64 %49, ptr %3, align 8
//L +   %50 = load i64, ptr %3, align 8
//L +   %51 = icmp eq i64 %1, 0
//L +   br i1 %51, label %52, label %60
//L +
//L + 52:                                               ; preds = %47
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %53 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %53, align 8
//L +   %54 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 12, ptr %54, align 8
//L +   %55 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %55, align 8
//L +   %56 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.5, ptr %56, align 8
//L +   %57 = getelementptr inbounds nuw i8, ptr %56, i32 8
//L +   store i64 1, ptr %57, align 8
//L +   %58 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.6, ptr %58, align 8
//L +   %59 = getelementptr inbounds nuw i8, ptr %58, i32 8
//L +   store i64 16, ptr %59, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 60:                                               ; preds = %47
//L +   %61 = icmp eq i64 %50, -9223372036854775808
//L +   %62 = icmp eq i64 %1, -1
//L +   %63 = and i1 %61, %62
//L +   br i1 %63, label %64, label %72
//L +
//L + 64:                                               ; preds = %60
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %65 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %65, align 8
//L +   %66 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 12, ptr %66, align 8
//L +   %67 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %67, align 8
//L +   %68 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.5, ptr %68, align 8
//L +   %69 = getelementptr inbounds nuw i8, ptr %68, i32 8
//L +   store i64 1, ptr %69, align 8
//L +   %70 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.1, ptr %70, align 8
//L +   %71 = getelementptr inbounds nuw i8, ptr %70, i32 8
//L +   store i64 16, ptr %71, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 72:                                               ; preds = %60
//L +   %73 = sdiv i64 %50, %1
//L +   store i64 %73, ptr %3, align 8
//L +   %74 = load i64, ptr %3, align 8
//L +   %75 = icmp eq i64 %1, 0
//L +   br i1 %75, label %76, label %84
//L +
//L + 76:                                               ; preds = %72
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %77 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %77, align 8
//L +   %78 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 13, ptr %78, align 8
//L +   %79 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %79, align 8
//L +   %80 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.7, ptr %80, align 8
//L +   %81 = getelementptr inbounds nuw i8, ptr %80, i32 8
//L +   store i64 1, ptr %81, align 8
//L +   %82 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.6, ptr %82, align 8
//L +   %83 = getelementptr inbounds nuw i8, ptr %82, i32 8
//L +   store i64 16, ptr %83, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 84:                                               ; preds = %72
//L +   %85 = icmp eq i64 %74, -9223372036854775808
//L +   %86 = icmp eq i64 %1, -1
//L +   %87 = and i1 %85, %86
//L +   br i1 %87, label %88, label %96
//L +
//L + 88:                                               ; preds = %84
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %89 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %89, align 8
//L +   %90 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 13, ptr %90, align 8
//L +   %91 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %91, align 8
//L +   %92 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.7, ptr %92, align 8
//L +   %93 = getelementptr inbounds nuw i8, ptr %92, i32 8
//L +   store i64 1, ptr %93, align 8
//L +   %94 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.1, ptr %94, align 8
//L +   %95 = getelementptr inbounds nuw i8, ptr %94, i32 8
//L +   store i64 16, ptr %95, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 96:                                               ; preds = %84
//L +   %97 = srem i64 %74, %1
//L +   store i64 %97, ptr %3, align 8
//L +   %98 = load i64, ptr %3, align 8
//L +   %99 = icmp uge i64 %1, 64
//L +   br i1 %99, label %100, label %108
//L +
//L + 100:                                              ; preds = %96
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %101 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %101, align 8
//L +   %102 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 14, ptr %102, align 8
//L +   %103 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %103, align 8
//L +   %104 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.8, ptr %104, align 8
//L +   %105 = getelementptr inbounds nuw i8, ptr %104, i32 8
//L +   store i64 2, ptr %105, align 8
//L +   %106 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.9, ptr %106, align 8
//L +   %107 = getelementptr inbounds nuw i8, ptr %106, i32 8
//L +   store i64 30, ptr %107, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 108:                                              ; preds = %96
//L +   %109 = shl i64 %98, %1
//L +   %110 = ashr i64 %98, 63
//L +   %111 = ashr i64 %109, 63
//L +   %112 = icmp ne i64 %110, %111
//L +   br i1 %112, label %113, label %121
//L +
//L + 113:                                              ; preds = %108
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %114 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %114, align 8
//L +   %115 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 14, ptr %115, align 8
//L +   %116 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %116, align 8
//L +   %117 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.8, ptr %117, align 8
//L +   %118 = getelementptr inbounds nuw i8, ptr %117, i32 8
//L +   store i64 2, ptr %118, align 8
//L +   %119 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.1, ptr %119, align 8
//L +   %120 = getelementptr inbounds nuw i8, ptr %119, i32 8
//L +   store i64 16, ptr %120, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 121:                                              ; preds = %108
//L +   store i64 %109, ptr %3, align 8
//L +   %122 = load i64, ptr %3, align 8
//L +   %123 = ashr i64 %122, %1
//L +   store i64 %123, ptr %3, align 8
//L +   %124 = load i64, ptr %3, align 8
//L +   %125 = icmp uge i64 %1, 64
//L +   br i1 %125, label %126, label %134
//L +
//L + 126:                                              ; preds = %121
//L +   store ptr @__srcc_str.2, ptr %4, align 8
//L +   %127 = getelementptr inbounds nuw i8, ptr %4, i32 8
//L +   store i64 19, ptr %127, align 8
//L +   %128 = getelementptr inbounds nuw i8, ptr %4, i32 16
//L +   store i64 16, ptr %128, align 8
//L +   %129 = getelementptr inbounds nuw i8, ptr %4, i32 24
//L +   store i64 5, ptr %129, align 8
//L +   %130 = getelementptr inbounds nuw i8, ptr %4, i32 32
//L +   store ptr @__srcc_str.10, ptr %130, align 8
//L +   %131 = getelementptr inbounds nuw i8, ptr %130, i32 8
//L +   store i64 3, ptr %131, align 8
//L +   %132 = getelementptr inbounds nuw i8, ptr %4, i32 48
//L +   store ptr @__srcc_str.9, ptr %132, align 8
//L +   %133 = getelementptr inbounds nuw i8, ptr %132, i32 8
//L +   store i64 30, ptr %133, align 8
//L +   call void @__src_int_arith_error(ptr %4)
//L +   unreachable
//L +
//L + 134:                                              ; preds = %121
//L +   %135 = shl i64 %124, %1
//L +   store i64 %135, ptr %3, align 8
//L +   %136 = load i64, ptr %3, align 8
//L +   %137 = lshr i64 %136, %1
//L +   store i64 %137, ptr %3, align 8
//L +   ret void
//L + }
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.sadd.with.overflow.i64(i64, i64) #0
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.ssub.with.overflow.i64(i64, i64) #0
//L +
//L + ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
//L + declare { i64, i1 } @llvm.smul.with.overflow.i64(i64, i64) #0
//L +
//L + attributes #0 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
//L +
//L + !llvm.module.flags = !{!0}
//L +
//L + !0 = !{i32 2, !"Debug Info Version", i32 3}
