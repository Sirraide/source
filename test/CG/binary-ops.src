// R %srcc --llvm %s
program test;

proc sink (int) {}
proc sink (bool) {}

proc arith_checked (int a, int b) {
    sink(a + b);
    sink(a - b);
    sink(a * b);

    sink(a / b);
    sink(a % b);
    sink(a :/ b);
    sink(a :% b);

    sink(a << b);
    sink(a <<< b);
}

proc arith (int a, int b) {
    sink(a +~ b);
    sink(a -~ b);
    sink(a *~ b);

    sink(a >> b);
    sink(a >>> b);

    sink(a & b);
    sink(a | b);

    sink(a < b);
    sink(a <= b);
    sink(a > b);
    sink(a >= b);

    sink(a <: b);
    sink(a <=: b);
    sink(a :> b);
    sink(a :>= b);

    sink(a == b);
    sink(a != b);
}

proc exp (int a, int b) {
    sink(a ** b);
}

// * define private fastcc void @_S13arith_checkedFviiE(i64 %0, i64 %1) {
// + entry:
// +   %2 = alloca i64, align 8
// +   %3 = alloca i64, align 8
// +   store i64 %0, ptr %2, align 4
// +   store i64 %1, ptr %3, align 4
// +   %l2sr = load i64, ptr %2, align 4
// +   %l2sr1 = load i64, ptr %3, align 4
// +   %4 = call { i64, i1 } @llvm.sadd.with.overflow.i64(i64 %l2sr, i64 %l2sr1)
// +   %5 = extractvalue { i64, i1 } %4, 1
// +   br i1 %5, label %arith.fail, label %arith.join
// +
// + arith.fail:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 8, i64 10, { ptr, i64 } { ptr @1, i64 1 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join:
// +   %6 = extractvalue { i64, i1 } %4, 0
// +   call fastcc void @_S4sinkFviE(i64 %6)
// +   %l2sr2 = load i64, ptr %2, align 4
// +   %l2sr3 = load i64, ptr %3, align 4
// +   %7 = call { i64, i1 } @llvm.ssub.with.overflow.i64(i64 %l2sr2, i64 %l2sr3)
// +   %8 = extractvalue { i64, i1 } %7, 1
// +   br i1 %8, label %arith.fail4, label %arith.join5
// +
// + arith.fail4:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 9, i64 10, { ptr, i64 } { ptr @3, i64 1 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join5:
// +   %9 = extractvalue { i64, i1 } %7, 0
// +   call fastcc void @_S4sinkFviE(i64 %9)
// +   %l2sr6 = load i64, ptr %2, align 4
// +   %l2sr7 = load i64, ptr %3, align 4
// +   %10 = call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %l2sr6, i64 %l2sr7)
// +   %11 = extractvalue { i64, i1 } %10, 1
// +   br i1 %11, label %arith.fail8, label %arith.join9
// +
// + arith.fail8:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 10, i64 10, { ptr, i64 } { ptr @4, i64 1 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join9:
// +   %12 = extractvalue { i64, i1 } %10, 0
// +   call fastcc void @_S4sinkFviE(i64 %12)
// +   %l2sr10 = load i64, ptr %2, align 4
// +   %l2sr11 = load i64, ptr %3, align 4
// +   %13 = icmp eq i64 %l2sr11, 0
// +   br i1 %13, label %arith.fail12, label %arith.join13
// +
// + arith.fail12:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 12, i64 10, { ptr, i64 } { ptr @5, i64 1 }, { ptr, i64 } { ptr @6, i64 16 })
// +   unreachable
// +
// + arith.join13:
// +   %14 = icmp eq i64 %l2sr10, -9223372036854775808
// +   %15 = icmp eq i64 %l2sr11, -1
// +   %16 = and i1 %14, %15
// +   br i1 %16, label %arith.fail14, label %arith.join15
// +
// + arith.fail14:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 12, i64 10, { ptr, i64 } { ptr @5, i64 1 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join15:
// +   %17 = sdiv i64 %l2sr10, %l2sr11
// +   call fastcc void @_S4sinkFviE(i64 %17)
// +   %l2sr16 = load i64, ptr %2, align 4
// +   %l2sr17 = load i64, ptr %3, align 4
// +   %18 = icmp eq i64 %l2sr17, 0
// +   br i1 %18, label %arith.fail18, label %arith.join19
// +
// + arith.fail18:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 13, i64 10, { ptr, i64 } { ptr @7, i64 1 }, { ptr, i64 } { ptr @6, i64 16 })
// +   unreachable
// +
// + arith.join19:
// +   %19 = icmp eq i64 %l2sr16, -9223372036854775808
// +   %20 = icmp eq i64 %l2sr17, -1
// +   %21 = and i1 %19, %20
// +   br i1 %21, label %arith.fail20, label %arith.join21
// +
// + arith.fail20:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 13, i64 10, { ptr, i64 } { ptr @7, i64 1 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join21:
// +   %22 = srem i64 %l2sr16, %l2sr17
// +   call fastcc void @_S4sinkFviE(i64 %22)
// +   %l2sr22 = load i64, ptr %2, align 4
// +   %l2sr23 = load i64, ptr %3, align 4
// +   %23 = icmp eq i64 %l2sr23, 0
// +   br i1 %23, label %arith.fail24, label %arith.join25
// +
// + arith.fail24:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 14, i64 10, { ptr, i64 } { ptr @8, i64 2 }, { ptr, i64 } { ptr @6, i64 16 })
// +   unreachable
// +
// + arith.join25:
// +   %24 = udiv i64 %l2sr22, %l2sr23
// +   call fastcc void @_S4sinkFviE(i64 %24)
// +   %l2sr26 = load i64, ptr %2, align 4
// +   %l2sr27 = load i64, ptr %3, align 4
// +   %25 = icmp eq i64 %l2sr27, 0
// +   br i1 %25, label %arith.fail28, label %arith.join29
// +
// + arith.fail28:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 15, i64 10, { ptr, i64 } { ptr @9, i64 2 }, { ptr, i64 } { ptr @6, i64 16 })
// +   unreachable
// +
// + arith.join29:
// +   %26 = urem i64 %l2sr26, %l2sr27
// +   call fastcc void @_S4sinkFviE(i64 %26)
// +   %l2sr30 = load i64, ptr %2, align 4
// +   %l2sr31 = load i64, ptr %3, align 4
// +   %27 = icmp uge i64 %l2sr31, 64
// +   br i1 %27, label %arith.fail32, label %arith.join33
// +
// + arith.fail32:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 17, i64 10, { ptr, i64 } { ptr @10, i64 2 }, { ptr, i64 } { ptr @11, i64 30 })
// +   unreachable
// +
// + arith.join33:
// +   %28 = shl i64 %l2sr30, %l2sr31
// +   %29 = ashr i64 %l2sr30, 63
// +   %30 = ashr i64 %28, 63
// +   %31 = icmp ne i64 %29, %30
// +   br i1 %31, label %arith.fail34, label %arith.join35
// +
// + arith.fail34:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 17, i64 10, { ptr, i64 } { ptr @10, i64 2 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join35:
// +   call fastcc void @_S4sinkFviE(i64 %28)
// +   %l2sr36 = load i64, ptr %2, align 4
// +   %l2sr37 = load i64, ptr %3, align 4
// +   %32 = icmp uge i64 %l2sr37, 64
// +   br i1 %32, label %arith.fail38, label %arith.join39
// +
// + arith.fail38:
// +   call void @__src_int_arith_error({ ptr, i64 } { ptr @0, i64 47 }, i64 18, i64 10, { ptr, i64 } { ptr @12, i64 3 }, { ptr, i64 } { ptr @11, i64 30 })
// +   unreachable
// +
// + arith.join39:
// +   %33 = shl i64 %l2sr36, %l2sr37
// +   call fastcc void @_S4sinkFviE(i64 %33)
// +   ret void
// + }

// * define private fastcc void @_S5arithFviiE(i64 %0, i64 %1) {
// + entry:
// +   %2 = alloca i64, align 8
// +   %3 = alloca i64, align 8
// +   store i64 %0, ptr %2, align 4
// +   store i64 %1, ptr %3, align 4
// +   %l2sr = load i64, ptr %2, align 4
// +   %l2sr1 = load i64, ptr %3, align 4
// +   %4 = add i64 %l2sr, %l2sr1
// +   call fastcc void @_S4sinkFviE(i64 %4)
// +   %l2sr2 = load i64, ptr %2, align 4
// +   %l2sr3 = load i64, ptr %3, align 4
// +   %5 = sub i64 %l2sr2, %l2sr3
// +   call fastcc void @_S4sinkFviE(i64 %5)
// +   %l2sr4 = load i64, ptr %2, align 4
// +   %l2sr5 = load i64, ptr %3, align 4
// +   %6 = mul i64 %l2sr4, %l2sr5
// +   call fastcc void @_S4sinkFviE(i64 %6)
// +   %l2sr6 = load i64, ptr %2, align 4
// +   %l2sr7 = load i64, ptr %3, align 4
// +   %7 = ashr i64 %l2sr6, %l2sr7
// +   call fastcc void @_S4sinkFviE(i64 %7)
// +   %l2sr8 = load i64, ptr %2, align 4
// +   %l2sr9 = load i64, ptr %3, align 4
// +   %8 = lshr i64 %l2sr8, %l2sr9
// +   call fastcc void @_S4sinkFviE(i64 %8)
// +   %l2sr10 = load i64, ptr %2, align 4
// +   %l2sr11 = load i64, ptr %3, align 4
// +   %9 = and i64 %l2sr10, %l2sr11
// +   call fastcc void @_S4sinkFviE(i64 %9)
// +   %l2sr12 = load i64, ptr %2, align 4
// +   %l2sr13 = load i64, ptr %3, align 4
// +   %10 = or i64 %l2sr12, %l2sr13
// +   call fastcc void @_S4sinkFviE(i64 %10)
// +   %l2sr14 = load i64, ptr %2, align 4
// +   %l2sr15 = load i64, ptr %3, align 4
// +   %slt = icmp slt i64 %l2sr14, %l2sr15
// +   call fastcc void @_S4sinkFvbE(i1 %slt)
// +   %l2sr16 = load i64, ptr %2, align 4
// +   %l2sr17 = load i64, ptr %3, align 4
// +   %sle = icmp sle i64 %l2sr16, %l2sr17
// +   call fastcc void @_S4sinkFvbE(i1 %sle)
// +   %l2sr18 = load i64, ptr %2, align 4
// +   %l2sr19 = load i64, ptr %3, align 4
// +   %sgt = icmp sgt i64 %l2sr18, %l2sr19
// +   call fastcc void @_S4sinkFvbE(i1 %sgt)
// +   %l2sr20 = load i64, ptr %2, align 4
// +   %l2sr21 = load i64, ptr %3, align 4
// +   %sge = icmp sge i64 %l2sr20, %l2sr21
// +   call fastcc void @_S4sinkFvbE(i1 %sge)
// +   %l2sr22 = load i64, ptr %2, align 4
// +   %l2sr23 = load i64, ptr %3, align 4
// +   %ult = icmp ult i64 %l2sr22, %l2sr23
// +   call fastcc void @_S4sinkFvbE(i1 %ult)
// +   %l2sr24 = load i64, ptr %2, align 4
// +   %l2sr25 = load i64, ptr %3, align 4
// +   %ule = icmp ule i64 %l2sr24, %l2sr25
// +   call fastcc void @_S4sinkFvbE(i1 %ule)
// +   %l2sr26 = load i64, ptr %2, align 4
// +   %l2sr27 = load i64, ptr %3, align 4
// +   %ugt = icmp ugt i64 %l2sr26, %l2sr27
// +   call fastcc void @_S4sinkFvbE(i1 %ugt)
// +   %l2sr28 = load i64, ptr %2, align 4
// +   %l2sr29 = load i64, ptr %3, align 4
// +   %uge = icmp uge i64 %l2sr28, %l2sr29
// +   call fastcc void @_S4sinkFvbE(i1 %uge)
// +   %l2sr30 = load i64, ptr %2, align 4
// +   %l2sr31 = load i64, ptr %3, align 4
// +   %eq = icmp eq i64 %l2sr30, %l2sr31
// +   call fastcc void @_S4sinkFvbE(i1 %eq)
// +   %l2sr32 = load i64, ptr %2, align 4
// +   %l2sr33 = load i64, ptr %3, align 4
// +   %ne = icmp ne i64 %l2sr32, %l2sr33
// +   call fastcc void @_S4sinkFvbE(i1 %ne)
// +   ret void
// + }

// * define private fastcc void @_S3expFviiE(i64 %0, i64 %1) {
// + entry:
// +   %2 = alloca i64, align 8
// +   %3 = alloca i64, align 8
// +   store i64 %0, ptr %2, align 4
// +   store i64 %1, ptr %3, align 4
// +   %l2sr = load i64, ptr %2, align 4
// +   %l2sr1 = load i64, ptr %3, align 4
// +   %4 = call i64 @__srcc_exp_i64(i64 %l2sr, i64 %l2sr1)
// +   call fastcc void @_S4sinkFviE(i64 %4)
// +   ret void
// + }

// * define private fastcc i64 @__srcc_exp_i64(i64 %0, i64 %1) #2 {
// + entry:
// +   %2 = icmp eq i64 %1, 0
// +   br i1 %2, label %3, label %4
// +
// + 3:
// +   ret i64 1
// +
// + 4:
// +   %5 = icmp eq i64 %0, 0
// +   br i1 %5, label %6, label %8
// +
// + 6:
// +   %7 = icmp slt i64 %1, 0
// +   br i1 %7, label %arith.fail, label %arith.join
// +
// + arith.fail:
// +   call void @__src_int_arith_error({ ptr, i64 } zeroinitializer, i64 0, i64 0, { ptr, i64 } { ptr @13, i64 2 }, { ptr, i64 } { ptr @6, i64 16 })
// +   unreachable
// +
// + arith.join:
// +   ret i64 0
// +
// + 8:
// +   %9 = icmp slt i64 %1, 0
// +   br i1 %9, label %10, label %18
// +
// + 10:
// +   %11 = icmp eq i64 %0, -1
// +   br i1 %11, label %12, label %15
// +
// + 12:
// +   %13 = trunc i64 %1 to i1
// +   %14 = select i1 %13, i64 -1, i64 1
// +   ret i64 %14
// +
// + 15:
// +   %16 = icmp eq i64 %0, 1
// +   %17 = select i1 %16, i64 1, i64 0
// +   ret i64 %17
// +
// + 18:
// +   %19 = icmp eq i64 %0, -9223372036854775808
// +   br i1 %19, label %arith.fail1, label %arith.join2
// +
// + arith.fail1:
// +   call void @__src_int_arith_error({ ptr, i64 } zeroinitializer, i64 0, i64 0, { ptr, i64 } { ptr @13, i64 2 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join2:
// +   br label %20
// +
// + 20:
// +   %21 = phi i64 [ %0, %arith.join2 ], [ %28, %arith.join4 ]
// +   %22 = phi i64 [ %1, %arith.join2 ], [ %29, %arith.join4 ]
// +   %23 = icmp eq i64 %22, 0
// +   br i1 %23, label %24, label %25
// +
// + 24:
// +   ret i64 %21
// +
// + 25:
// +   %26 = call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %21, i64 %0)
// +   %27 = extractvalue { i64, i1 } %26, 1
// +   br i1 %27, label %arith.fail3, label %arith.join4
// +
// + arith.fail3:
// +   call void @__src_int_arith_error({ ptr, i64 } zeroinitializer, i64 0, i64 0, { ptr, i64 } { ptr @4, i64 1 }, { ptr, i64 } { ptr @2, i64 16 })
// +   unreachable
// +
// + arith.join4:
// +   %28 = extractvalue { i64, i1 } %26, 0
// +   %29 = sub i64 %22, 1
// +   br label %20
// + }
