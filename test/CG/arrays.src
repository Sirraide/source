// R %srcc --ir --target x86_64-unknown-linux %s
program test;

// Broadcast initialisation.
proc a {
    var x1 = int[4](0);
    var x2 = int[7](1);
    var x3 = int[200](100);
    var x4 = i16[5](20);
}

// Exhaustive elementwise initialisation.
proc b {
    var x1 = int[1](0);
    var x2 = int[1](1);
    var x3 = int[1](100);
    var x4 = int[1](200);
    var x5 = int[4](1, 2, 3, 4);
    var x6 = i16[10](1, 2, 3, 4, 5, 6, 7, 8, 9, 10);
    var x7 = int[3](40, 29, 123);

    // These are technically non-exhaustive, but the broadcast initialiser
    // is only used for a single element, and because of how codegen handles
    // this we don't end up emitting a loop for this.
    var x1 = int[3](0, 1);
    var x2 = int[3](2, 3);
}

// Non-exhaustive elementwise initialisation.
proc c {
    var x3 = int[10](1, 2, 3, 4, 5);
    var x4 = i16[200](61, 75, 230, 40, 25);
}

// * proc __src_assert_fail (ptr) external fastcc;
// +
// + proc __src_int_arith_error (ptr) external fastcc;
// +
// + proc __src_main external fastcc {
// + entry:
// +     ret
// + }
// +
// + proc _S1aFvE private fastcc {
// +     #0 = 32, align 8
// +     #1 = 56, align 8
// +     #2 = 1600, align 8
// +     #3 = 10, align 2
// +
// + entry:
// +     br bb1(0)
// +
// + bb1(i64 %0):
// +     %1 = icmp eq i64 %0, 4
// +     br %1 to bb3(0) else bb2
// +
// + bb2:
// +     %2 = mul i64 %0, 8
// +     %3 = ptradd #0, i64 %2
// +     store %3, i64 0, align 8
// +     %4 = add i64 %0, 1
// +     br bb1(%4)
// +
// + bb3(i64 %5):
// +     %6 = icmp eq i64 %5, 7
// +     br %6 to bb5(0) else bb4
// +
// + bb4:
// +     %7 = mul i64 %5, 8
// +     %8 = ptradd #1, i64 %7
// +     store %8, i64 1, align 8
// +     %9 = add i64 %5, 1
// +     br bb3(%9)
// +
// + bb5(i64 %10):
// +     %11 = icmp eq i64 %10, 200
// +     br %11 to bb7(0) else bb6
// +
// + bb6:
// +     %12 = mul i64 %10, 8
// +     %13 = ptradd #2, i64 %12
// +     store %13, i64 100, align 8
// +     %14 = add i64 %10, 1
// +     br bb5(%14)
// +
// + bb7(i64 %15):
// +     %16 = icmp eq i64 %15, 5
// +     br %16 to bb9 else bb8
// +
// + bb8:
// +     %17 = mul i64 %15, 2
// +     %18 = ptradd #3, i64 %17
// +     store %18, i16 20, align 2
// +     %19 = add i64 %15, 1
// +     br bb7(%19)
// +
// + bb9:
// +     ret
// + }
// +
// + proc _S1bFvE private fastcc {
// +     #0 = 8, align 8
// +     #1 = 8, align 8
// +     #2 = 8, align 8
// +     #3 = 8, align 8
// +     #4 = 32, align 8
// +     #5 = 20, align 2
// +     #6 = 24, align 8
// +     #7 = 24, align 8
// +     #8 = 24, align 8
// +
// + entry:
// +     store #0, i64 0, align 8
// +     store #1, i64 1, align 8
// +     store #2, i64 100, align 8
// +     store #3, i64 200, align 8
// +     store #4, i64 1, align 8
// +     %0 = ptradd #4, 8
// +     store %0, i64 2, align 8
// +     %1 = ptradd %0, 8
// +     store %1, i64 3, align 8
// +     %2 = ptradd %1, 8
// +     store %2, i64 4, align 8
// +     store #5, i16 1, align 2
// +     %3 = ptradd #5, 2
// +     store %3, i16 2, align 2
// +     %4 = ptradd %3, 2
// +     store %4, i16 3, align 2
// +     %5 = ptradd %4, 2
// +     store %5, i16 4, align 2
// +     %6 = ptradd %5, 2
// +     store %6, i16 5, align 2
// +     %7 = ptradd %6, 2
// +     store %7, i16 6, align 2
// +     %8 = ptradd %7, 2
// +     store %8, i16 7, align 2
// +     %9 = ptradd %8, 2
// +     store %9, i16 8, align 2
// +     %10 = ptradd %9, 2
// +     store %10, i16 9, align 2
// +     %11 = ptradd %10, 2
// +     store %11, i16 10, align 2
// +     store #6, i64 40, align 8
// +     %12 = ptradd #6, 8
// +     store %12, i64 29, align 8
// +     %13 = ptradd %12, 8
// +     store %13, i64 123, align 8
// +     store #7, i64 0, align 8
// +     %14 = ptradd #7, 8
// +     store %14, i64 1, align 8
// +     %15 = ptradd %14, 8
// +     store %15, i64 0, align 8
// +     store #8, i64 2, align 8
// +     %16 = ptradd #8, 8
// +     store %16, i64 3, align 8
// +     %17 = ptradd %16, 8
// +     store %17, i64 0, align 8
// +     ret
// + }
// +
// + proc _S1cFvE private fastcc {
// +     #0 = 80, align 8
// +     #1 = 400, align 2
// +
// + entry:
// +     store #0, i64 1, align 8
// +     %0 = ptradd #0, 8
// +     store %0, i64 2, align 8
// +     %1 = ptradd %0, 8
// +     store %1, i64 3, align 8
// +     %2 = ptradd %1, 8
// +     store %2, i64 4, align 8
// +     %3 = ptradd %2, 8
// +     store %3, i64 5, align 8
// +     %4 = ptradd %3, 8
// +     store %4, i64 0, align 8
// +     %5 = ptradd %4, 8
// +     set ptr %5, i8 0, i64 32
// +     store #1, i16 61, align 2
// +     %6 = ptradd #1, 2
// +     store %6, i16 75, align 2
// +     %7 = ptradd %6, 2
// +     store %7, i16 230, align 2
// +     %8 = ptradd %7, 2
// +     store %8, i16 40, align 2
// +     %9 = ptradd %8, 2
// +     store %9, i16 25, align 2
// +     %10 = ptradd %9, 2
// +     store %10, i16 0, align 2
// +     %11 = ptradd %10, 2
// +     set ptr %11, i8 0, i64 388
// +     ret
// + }
